{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d95942f",
   "metadata": {},
   "source": [
    "## 1. Setup Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Verifica GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03769154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Monta Google Drive (per checkpoint persistenti)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Drive montato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Estrai progetto da zip\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "ZIP_PATH = '/content/drive/MyDrive/phonemeRef.zip'\n",
    "EXTRACT_PATH = '/content/DeepLearning-Phoneme'\n",
    "\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå File non trovato: {ZIP_PATH}\\nCarica phonemeRef.zip su Google Drive\")\n",
    "\n",
    "print(f\"üì¶ Estrazione {ZIP_PATH}...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')\n",
    "\n",
    "# Trova cartella estratta\n",
    "extracted = [f for f in os.listdir('/content/') if os.path.isdir(f'/content/{f}') and 'Phoneme' in f]\n",
    "if extracted:\n",
    "    EXTRACT_PATH = f'/content/{extracted[0]}'\n",
    "\n",
    "os.chdir(EXTRACT_PATH)\n",
    "print(f\"‚úÖ Progetto in: {EXTRACT_PATH}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Installa dipendenze\n",
    "!pip install -q transformers datasets evaluate jiwer accelerate soundfile librosa pyyaml tqdm audiomentations\n",
    "\n",
    "# Torchcodec richiesto per decodifica audio nei datasets HuggingFace\n",
    "!pip install -q torchcodec\n",
    "\n",
    "print(\"\\n‚úÖ Dipendenze installate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a283b8c",
   "metadata": {},
   "source": [
    "## 2. Preparazione Dataset\n",
    "\n",
    "**Due opzioni:**\n",
    "- **A) Usa dataset esistente** (pi√π veloce) - se il dataset augmentato √® gi√† nel zip\n",
    "- **B) Rigenera augmentation** (pi√π lento, ~30 min) - se vuoi ricreare i dati\n",
    "\n",
    "Esegui le celle della sezione che preferisci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba1c8e",
   "metadata": {},
   "source": [
    "### 2A. Usa Dataset Esistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7639dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A.1 Carica e analizza dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Trova il dataset\n",
    "DATASET_OPTIONS = [\n",
    "    'data/processed/combined_augmented.csv'\n",
    "]\n",
    "\n",
    "DATASET_CSV = None\n",
    "for opt in DATASET_OPTIONS:\n",
    "    if Path(opt).exists():\n",
    "        DATASET_CSV = opt\n",
    "        break\n",
    "\n",
    "if not DATASET_CSV:\n",
    "    raise FileNotFoundError(\"‚ùå Nessun dataset trovato! Esegui la sezione 2B.\")\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
    "print(f\"   Samples: {len(df):,}\")\n",
    "print(f\"\\n=== Distribuzione ===\")\n",
    "if 'source' in df.columns:\n",
    "    print(df['source'].value_counts())\n",
    "if 'is_correct' in df.columns:\n",
    "    print(f\"\\n=== Corretti vs Errori ===\")\n",
    "    print(df['is_correct'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2682113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A.2 Verifica qualit√† IPA (cerca placeholder invalidi E annotazioni)\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "# 1. Cerca IPA invalidi (placeholder [word])\n",
    "placeholder_mask = df['ipa_clean'].str.contains(r'^\\[.*\\]$', regex=True, na=False)\n",
    "\n",
    "# 2. Cerca annotazioni problematiche (adj., n., v., etc.) - NUOVA verifica!\n",
    "annotation_mask = df['ipa_clean'].str.contains(\n",
    "    r'adj\\.|n\\.|v\\.|adv\\.|interj\\.|for \\d|unstressed|stressed|esp\\.|also|Brit\\.|;',\n",
    "    regex=True, na=False\n",
    ")\n",
    "\n",
    "# 3. IPA troppo corti (< 2 caratteri)\n",
    "short_mask = df['ipa_clean'].str.len() < 2\n",
    "\n",
    "invalid_mask = placeholder_mask | annotation_mask | short_mask\n",
    "invalid_count = invalid_mask.sum()\n",
    "\n",
    "print(f\"üîç Analisi qualit√† IPA:\")\n",
    "print(f\"   Totale samples: {len(df):,}\")\n",
    "print(f\"   IPA placeholder [word]: {placeholder_mask.sum():,}\")\n",
    "print(f\"   IPA con annotazioni (adj., v., etc.): {annotation_mask.sum():,}\")\n",
    "print(f\"   IPA troppo corti (<2): {short_mask.sum():,}\")\n",
    "print(f\"   Totale invalidi: {invalid_count:,} ({100*invalid_count/len(df):.1f}%)\")\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è ATTENZIONE: {invalid_count} samples hanno IPA problematici!\")\n",
    "    \n",
    "    # Mostra esempi\n",
    "    print(\"\\n   Esempi di IPA invalidi:\")\n",
    "    examples = df[invalid_mask][['word', 'ipa_clean']].head(10)\n",
    "    for _, row in examples.iterrows():\n",
    "        print(f\"   - {row['word']}: '{row['ipa_clean']}'\")\n",
    "    \n",
    "    # Rimuovi invalidi\n",
    "    df_clean = df[~invalid_mask].copy()\n",
    "    DATASET_CLEAN = 'data/processed/phonemeref_clean.csv'\n",
    "    df_clean.to_csv(DATASET_CLEAN, index=False)\n",
    "    print(f\"\\n‚úÖ Dataset pulito salvato: {DATASET_CLEAN}\")\n",
    "    print(f\"   Samples validi: {len(df_clean):,}\")\n",
    "    DATASET_CSV = DATASET_CLEAN\n",
    "else:\n",
    "    print(\"\\n‚úÖ Tutti gli IPA sono validi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af98d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A.3 Fix path e rimuovi file mancanti\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "def fix_path(path_str):\n",
    "    \"\"\"Converte path Windows in path Colab.\"\"\"\n",
    "    path_str = str(path_str).replace('\\\\', '/')\n",
    "    \n",
    "    # Se √® gi√† un path relativo corretto (data/...), usalo\n",
    "    if path_str.startswith('data/'):\n",
    "        return path_str\n",
    "    \n",
    "    # Se inizia con 'audio/audio/' (caso speciale per parola 'audio')\n",
    "    if path_str.startswith('audio/audio/'):\n",
    "        return 'data/raw/phonemeref_data/' + path_str\n",
    "    \n",
    "    # Se inizia con 'audio/' (path relativo senza prefisso)\n",
    "    if path_str.startswith('audio/'):\n",
    "        return 'data/raw/phonemeref_data/' + path_str\n",
    "    \n",
    "    # Se contiene 'audio/' ma non 'data/', aggiungi il prefisso corretto\n",
    "    if '/audio/' in path_str:\n",
    "        idx = path_str.find('/audio/')\n",
    "        return 'data/raw/phonemeref_data' + path_str[idx:]\n",
    "    \n",
    "    # Se contiene path Windows assoluto con 'data/'\n",
    "    if 'data/' in path_str:\n",
    "        idx = path_str.find('data/')\n",
    "        return path_str[idx:]\n",
    "    \n",
    "    # Se contiene 'DeepLearning-Phoneme/'\n",
    "    if 'DeepLearning-Phoneme/' in path_str:\n",
    "        idx = path_str.find('DeepLearning-Phoneme/')\n",
    "        path_str = path_str[idx + len('DeepLearning-Phoneme/'):]\n",
    "        if not path_str.startswith('data/'):\n",
    "            path_str = 'data/raw/phonemeref_data/' + path_str\n",
    "        return path_str\n",
    "    \n",
    "    return path_str\n",
    "\n",
    "# Fix path\n",
    "df['audio_path'] = df['audio_path'].apply(fix_path)\n",
    "\n",
    "# === RIMUOVI FILE MANCANTI ===\n",
    "print(\"üîç Verifica esistenza file audio...\")\n",
    "missing_files = []\n",
    "existing_mask = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Checking files\"):\n",
    "    exists = Path(row['audio_path']).exists()\n",
    "    existing_mask.append(exists)\n",
    "    if not exists:\n",
    "        missing_files.append((row.get('word', '?'), row['audio_path']))\n",
    "\n",
    "existing_mask = pd.Series(existing_mask, index=df.index)\n",
    "n_missing = len(missing_files)\n",
    "n_total = len(df)\n",
    "\n",
    "print(f\"\\nüìä Risultato verifica:\")\n",
    "print(f\"   Totale samples: {n_total:,}\")\n",
    "print(f\"   File esistenti: {n_total - n_missing:,} ({100*(n_total-n_missing)/n_total:.1f}%)\")\n",
    "print(f\"   File mancanti: {n_missing:,} ({100*n_missing/n_total:.1f}%)\")\n",
    "\n",
    "if n_missing > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Esempi file mancanti:\")\n",
    "    for word, path in missing_files[:10]:\n",
    "        print(f\"   ‚ùå {word}: {path}\")\n",
    "    \n",
    "    # Rimuovi file mancanti\n",
    "    df_clean = df[existing_mask].copy()\n",
    "    print(f\"\\n‚úÖ Rimossi {n_missing} samples con file mancanti\")\n",
    "    print(f\"   Dataset finale: {len(df_clean):,} samples\")\n",
    "    df = df_clean\n",
    "else:\n",
    "    print(\"\\n‚úÖ Tutti i file audio esistono!\")\n",
    "\n",
    "# Verifica distribuzione finale\n",
    "if 'source' in df.columns:\n",
    "    print(f\"\\nüìä Distribuzione finale:\")\n",
    "    print(df['source'].value_counts())\n",
    "\n",
    "# Salva\n",
    "DATASET_FINAL = 'data/processed/phonemeref_ready.csv'\n",
    "df.to_csv(DATASET_FINAL, index=False)\n",
    "print(f\"\\n‚úÖ Dataset pronto: {DATASET_FINAL}\")\n",
    "DATASET_CSV = DATASET_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A.4 Verifica vocab.json\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "vocab_path = Path('data/processed/vocab.json')\n",
    "if vocab_path.exists():\n",
    "    with open(vocab_path, encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Vocab: {len(vocab)} simboli\")\n",
    "    \n",
    "    # Caratteri speciali attesi\n",
    "    special = ['[PAD]', '[UNK]', '|']\n",
    "    \n",
    "    # Caratteri non-IPA problematici\n",
    "    non_ipa = []\n",
    "    ipa_chars = []\n",
    "    for char in vocab.keys():\n",
    "        if char in special:\n",
    "            continue\n",
    "        # Controlla se √® un carattere IPA valido (usando regex)\n",
    "        if len(char) == 1 and char.isalpha() and not char.isascii():\n",
    "            ipa_chars.append(char)\n",
    "        elif char in ['Àà', 'Àå', 'Àê', ' ≥', \"'\", '-', ' ']:  # Accenti e simboli IPA\n",
    "            ipa_chars.append(char)\n",
    "        elif char.lower() in 'abcdefghijklmnopqrstuvwxyz':  # Lettere ASCII (ok per IPA)\n",
    "            ipa_chars.append(char)\n",
    "        else:\n",
    "            non_ipa.append(char)\n",
    "    \n",
    "    print(f\"\\n   Caratteri speciali: {special}\")\n",
    "    print(f\"   Caratteri IPA: {len(ipa_chars)}\")\n",
    "    \n",
    "    if non_ipa:\n",
    "        print(f\"\\n   ‚ö†Ô∏è Caratteri sospetti: {non_ipa}\")\n",
    "        print(\"      Verifica che siano realmente parte dell'IPA!\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚úÖ Tutti i caratteri sembrano IPA validi\")\n",
    "    \n",
    "    # Mostra alcuni caratteri\n",
    "    print(f\"\\n   Esempio simboli: {list(vocab.keys())[3:15]}...\")\n",
    "else:\n",
    "    print(\"‚ùå vocab.json non trovato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9a92c",
   "metadata": {},
   "source": [
    "### 2B. Rigenera Dataset Augmentato (Opzionale)\n",
    "\n",
    "Esegui questa sezione solo se vuoi ricreare il dataset da zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05339482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B.1 Rigenera dataset augmentato\n",
    "# ‚ö†Ô∏è ATTENZIONE: richiede ~10-15 minuti!\n",
    "\n",
    "RIGENERA = False  # Cambia in True per rigenerare\n",
    "\n",
    "if RIGENERA:\n",
    "    print(\"üîÑ Rigenerazione dataset augmentato...\")\n",
    "    print(\"   Questo richieder√† ~10-15 minuti.\")\n",
    "    \n",
    "    # Solo acoustic augmentation (no TTS)\n",
    "    !python scripts/build_augmented_dataset.py \\\n",
    "        --input data/processed/phonemeref_processed.csv \\\n",
    "        --output data/processed/phonemeref_augmented.csv \\\n",
    "        --num-variants 2\n",
    "    \n",
    "    DATASET_CSV = 'data/processed/phonemeref_augmented.csv'\n",
    "    print(f\"\\n‚úÖ Dataset rigenerato: {DATASET_CSV}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Rigenerazione saltata. Imposta RIGENERA=True per eseguire.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b28b2",
   "metadata": {},
   "source": [
    "## 3. Configurazione Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33060d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Configurazione (ottimizzata per Tesla T4)\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# === CONFIGURAZIONE PRINCIPALE ===\n",
    "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/phoneme_training_v2'\n",
    "# DATASET_CSV √® definito nella sezione 2\n",
    "\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'model': {\n",
    "        'name': 'microsoft/wavlm-large',\n",
    "        'freeze_feature_encoder': True  # Riduce VRAM, training pi√π veloce\n",
    "    },\n",
    "    'data': {\n",
    "        'csv_path': DATASET_CSV,\n",
    "        'vocab_path': 'data/processed/vocab.json',\n",
    "        'audio_base_path': '.',\n",
    "        'val_size': 0.05,\n",
    "        'test_size': 0.05,\n",
    "        'sampling_rate': 16000\n",
    "    },\n",
    "    'training': {\n",
    "        'output_dir': DRIVE_OUTPUT_DIR,\n",
    "        'num_train_epochs': 10,\n",
    "        \n",
    "        # === BATCH (ottimizzato per T4 15GB VRAM) ===\n",
    "        'per_device_train_batch_size': 8,    # Max stabile su T4\n",
    "        'per_device_eval_batch_size': 8,\n",
    "        'gradient_accumulation_steps': 2,     # Effettivo: 8*2=16\n",
    "        \n",
    "        # === DATALOADER (Colab-safe) ===\n",
    "        'dataloader_num_workers': 0,          # Evita memory leak su Colab\n",
    "        'dataloader_pin_memory': False,       # Non necessario con workers=0\n",
    "        \n",
    "        # === OPTIMIZER ===\n",
    "        'learning_rate': 3e-5,\n",
    "        'warmup_steps': 500,\n",
    "        'weight_decay': 0.01,\n",
    "        'optim': 'adamw_torch',\n",
    "        \n",
    "        # === GRADIENT CLIPPING (CRITICO per stabilit√†!) ===\n",
    "        'max_grad_norm': 1.0,                 # Previene gradient explosion\n",
    "        \n",
    "        # === MIXED PRECISION (T4 supporta FP16) ===\n",
    "        'fp16': True,                         # ~2x speedup su T4\n",
    "        'bf16': False,                        # T4 non supporta BF16\n",
    "        \n",
    "        # === EVAL/SAVE STRATEGY ===\n",
    "        'eval_strategy': 'epoch',             # Valida ogni epoca (pi√π stabile)\n",
    "        'save_strategy': 'epoch',             # Salva ogni epoca su Drive\n",
    "        'save_total_limit': 3,                # Mantieni ultimi 3 checkpoint\n",
    "        'load_best_model_at_end': True,\n",
    "        'metric_for_best_model': 'per',\n",
    "        'greater_is_better': False,\n",
    "        \n",
    "        # === LOGGING ===\n",
    "        'logging_steps': 100,                 # Log ogni 100 steps\n",
    "        'disable_tqdm': False,                # TQDM attivo per progress bar\n",
    "        \n",
    "        # === OTTIMIZZAZIONI ===\n",
    "        'group_by_length': True,              # Raggruppa audio simili (meno padding)\n",
    "        'gradient_checkpointing': False,      # Non necessario con batch 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crea directory output\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Salva config\n",
    "with open('configs/training_config_colab.yaml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìã CONFIGURAZIONE TRAINING (T4 Optimized)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Output: {DRIVE_OUTPUT_DIR}\")\n",
    "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
    "print(f\"üî¢ Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"üì¶ Batch: {config['training']['per_device_train_batch_size']} x {config['training']['gradient_accumulation_steps']} = {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"üìà LR: {config['training']['learning_rate']}\")\n",
    "print(f\"‚ö° FP16: {config['training']['fp16']}\")\n",
    "print(f\"üîí Gradient Clipping: {config['training']['max_grad_norm']}\")\n",
    "print(f\"üìä TQDM: Enabled\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Verifica checkpoint esistenti e stato training\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "output_dir = Path(DRIVE_OUTPUT_DIR)\n",
    "checkpoints = []\n",
    "\n",
    "if output_dir.exists():\n",
    "    checkpoints = sorted([\n",
    "        d for d in output_dir.iterdir() \n",
    "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
    "    ])\n",
    "\n",
    "print(f\"üìÅ Output: {output_dir}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if checkpoints:\n",
    "    print(f\"‚úÖ {len(checkpoints)} checkpoint trovati:\")\n",
    "    \n",
    "    last_epoch = 0\n",
    "    last_step = 0\n",
    "    best_per = None\n",
    "    \n",
    "    for cp in checkpoints[-3:]:  # Ultimi 3\n",
    "        state_file = cp / \"trainer_state.json\"\n",
    "        if state_file.exists():\n",
    "            with open(state_file) as f:\n",
    "                state = json.load(f)\n",
    "            epoch = state.get('epoch', 0)\n",
    "            step = state.get('global_step', 0)\n",
    "            best = state.get('best_metric', None)\n",
    "            max_steps = state.get('max_steps', '?')\n",
    "            \n",
    "            last_epoch = max(last_epoch, epoch)\n",
    "            last_step = max(last_step, step)\n",
    "            if best:\n",
    "                best_per = best\n",
    "            \n",
    "            info = f\"Epoch {epoch:.1f}, Step {step}/{max_steps}\"\n",
    "            if best:\n",
    "                info += f\", Best PER: {best:.4f}\"\n",
    "            print(f\"   üìÅ {cp.name}: {info}\")\n",
    "    \n",
    "    # === ANALISI STATO ===\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä ANALISI STATO TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    target_epochs = config['training']['num_train_epochs']\n",
    "    \n",
    "    print(f\"   Ultima epoch salvata: {last_epoch}\")\n",
    "    print(f\"   Epochs configurate: {target_epochs}\")\n",
    "    print(f\"   Ultimo step: {last_step}\")\n",
    "    if best_per:\n",
    "        print(f\"   Miglior PER: {best_per*100:.2f}%\")\n",
    "    \n",
    "    if last_epoch >= target_epochs:\n",
    "        print(f\"\\n‚ö†Ô∏è TRAINING GI√Ä COMPLETATO!\")\n",
    "        print(f\"   Il checkpoint √® a epoch {last_epoch}, target √® {target_epochs}\")\n",
    "        print(f\"\\n   OPZIONI:\")\n",
    "        print(f\"   1. Aumenta 'num_train_epochs' nella cella 3.1 (es. {int(target_epochs + 5)})\")\n",
    "        print(f\"   2. Elimina i checkpoint per ricominciare da zero:\")\n",
    "        print(f\"      !rm -rf {DRIVE_OUTPUT_DIR}/checkpoint-*\")\n",
    "    else:\n",
    "        remaining = target_epochs - last_epoch\n",
    "        print(f\"\\n‚úÖ Training pu√≤ continuare per {remaining:.0f} epoche\")\n",
    "        print(f\"   (da epoch {last_epoch} a {target_epochs})\")\n",
    "else:\n",
    "    print(\"‚ùå Nessun checkpoint - Training partir√† da zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cc733",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aaf132",
   "metadata": {},
   "source": [
    "### üóëÔ∏è Pulizia Checkpoint Corrotti (Se Necessario)\n",
    "\n",
    "Esegui questa cella SOLO se:\n",
    "- La diagnostica (cella precedente) ha rilevato un modello rotto\n",
    "- Vuoi ricominciare il training da zero\n",
    "- Hai checkpoint con PER > 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec174090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Elimina checkpoint e final_model corrotti\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚ö†Ô∏è ATTENZIONE: Cambia in True per eliminare\n",
    "DELETE_CORRUPTED = False\n",
    "\n",
    "if DELETE_CORRUPTED:\n",
    "    drive_path = Path(DRIVE_OUTPUT_DIR)\n",
    "    \n",
    "    if drive_path.exists():\n",
    "        print(f\"üóëÔ∏è Eliminazione contenuto: {drive_path}\")\n",
    "        \n",
    "        # Elimina checkpoint\n",
    "        checkpoints = list(drive_path.glob(\"checkpoint-*\"))\n",
    "        for cp in checkpoints:\n",
    "            if cp.is_dir():\n",
    "                shutil.rmtree(cp)\n",
    "                print(f\"   ‚úì Eliminato {cp.name}\")\n",
    "        \n",
    "        # Elimina final_model\n",
    "        final_model = drive_path / \"final_model\"\n",
    "        if final_model.exists():\n",
    "            shutil.rmtree(final_model)\n",
    "            print(f\"   ‚úì Eliminato final_model\")\n",
    "        \n",
    "        # Mantieni solo config e log\n",
    "        print(f\"\\n‚úÖ Pulizia completata!\")\n",
    "        print(f\"   Il training ripartir√† da zero alla prossima esecuzione\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Directory non trovata: {drive_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Pulizia disabilitata. Imposta DELETE_CORRUPTED=True per eliminare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Avvia Training\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Silenzia log\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# === OPZIONI ===\n",
    "RESUME = \"auto\"  # \"auto\", True, False\n",
    "\n",
    "# Rileva checkpoint\n",
    "drive_path = Path(DRIVE_OUTPUT_DIR)\n",
    "existing_checkpoints = []\n",
    "if drive_path.exists():\n",
    "    existing_checkpoints = sorted([\n",
    "        d for d in drive_path.iterdir() \n",
    "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
    "    ])\n",
    "\n",
    "# Determina resume\n",
    "if RESUME == \"auto\":\n",
    "    do_resume = len(existing_checkpoints) > 0\n",
    "elif RESUME:\n",
    "    do_resume = True\n",
    "else:\n",
    "    do_resume = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ AVVIO TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
    "print(f\"üìÅ Output: {DRIVE_OUTPUT_DIR}\")\n",
    "print(f\"üîÑ Resume: {do_resume}\")\n",
    "if existing_checkpoints:\n",
    "    print(f\"üìç Ultimo: {existing_checkpoints[-1].name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comando\n",
    "cmd = f\"python scripts/03_train.py --config configs/training_config_colab.yaml --data-csv {DATASET_CSV}\"\n",
    "if do_resume:\n",
    "    cmd += \" --resume\"\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccca1b8",
   "metadata": {},
   "source": [
    "## 5. Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af181c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Visualizza curve di training\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Trova trainer_state.json\n",
    "state_path = None\n",
    "for loc in [\n",
    "    Path(DRIVE_OUTPUT_DIR) / 'final_model' / 'trainer_state.json',\n",
    "    Path(DRIVE_OUTPUT_DIR) / 'trainer_state.json',\n",
    "]:\n",
    "    if loc.exists():\n",
    "        state_path = loc\n",
    "        break\n",
    "\n",
    "# Cerca anche nell'ultimo checkpoint\n",
    "if not state_path:\n",
    "    checkpoints = sorted([\n",
    "        d for d in Path(DRIVE_OUTPUT_DIR).iterdir() \n",
    "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
    "    ]) if Path(DRIVE_OUTPUT_DIR).exists() else []\n",
    "    if checkpoints:\n",
    "        state_path = checkpoints[-1] / 'trainer_state.json'\n",
    "\n",
    "if state_path and state_path.exists():\n",
    "    with open(state_path) as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    log_history = state.get('log_history', [])\n",
    "    \n",
    "    # Estrai metriche\n",
    "    train_loss = [(h['step'], h['loss']) for h in log_history if 'loss' in h and 'eval_loss' not in h]\n",
    "    eval_loss = [(h['step'], h['eval_loss']) for h in log_history if 'eval_loss' in h]\n",
    "    eval_per = [(h['step'], h['eval_per']) for h in log_history if 'eval_per' in h]\n",
    "    grad_norm = [(h['step'], h['grad_norm']) for h in log_history if 'grad_norm' in h and h.get('grad_norm') is not None]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    if train_loss:\n",
    "        steps, losses = zip(*train_loss)\n",
    "        axes[0,0].plot(steps, losses, 'b-', alpha=0.7)\n",
    "        axes[0,0].set_xlabel('Step')\n",
    "        axes[0,0].set_ylabel('Loss')\n",
    "        axes[0,0].set_title('Training Loss')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    if eval_loss:\n",
    "        steps, losses = zip(*eval_loss)\n",
    "        axes[0,1].plot(steps, losses, 'r-o')\n",
    "        axes[0,1].set_xlabel('Step')\n",
    "        axes[0,1].set_ylabel('Eval Loss')\n",
    "        axes[0,1].set_title('Validation Loss')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    if eval_per:\n",
    "        steps, pers = zip(*eval_per)\n",
    "        axes[1,0].plot(steps, [p*100 for p in pers], 'g-o')\n",
    "        axes[1,0].set_xlabel('Step')\n",
    "        axes[1,0].set_ylabel('PER (%)')\n",
    "        axes[1,0].set_title('Phoneme Error Rate')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    if grad_norm:\n",
    "        steps, norms = zip(*grad_norm)\n",
    "        axes[1,1].plot(steps, norms, 'm-', alpha=0.7)\n",
    "        axes[1,1].set_xlabel('Step')\n",
    "        axes[1,1].set_ylabel('Gradient Norm')\n",
    "        axes[1,1].set_title('Gradient Norm (check for explosion)')\n",
    "        axes[1,1].axhline(y=1.0, color='r', linestyle='--', label='max_grad_norm')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{DRIVE_OUTPUT_DIR}/training_curves.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    if eval_per:\n",
    "        best_per = min(pers)\n",
    "        print(f\"\\nüèÜ Migliore PER: {best_per*100:.2f}%\")\n",
    "    \n",
    "    # Check for gradient explosion\n",
    "    if grad_norm:\n",
    "        max_norm = max(norms)\n",
    "        if max_norm > 10:\n",
    "            print(f\"\\n‚ö†Ô∏è ATTENZIONE: Gradient norm max={max_norm:.2f} - possibile instabilit√†!\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Gradient norm stabile (max={max_norm:.2f})\")\n",
    "else:\n",
    "    print(\"‚ùå trainer_state.json non trovato - training non ancora completato?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test su alcuni samples (preferisci originali, ma accetta qualsiasi)\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "if 'source' in df.columns:\n",
    "    original_df = df[df['source'] == 'original']\n",
    "    if len(original_df) >= 5:\n",
    "        test_samples = original_df.sample(5, random_state=42)\n",
    "    elif len(original_df) > 0:\n",
    "        test_samples = original_df  # Usa tutti gli originali disponibili\n",
    "    else:\n",
    "        # Nessun 'original', prova fonti non augmentate\n",
    "        non_aug_sources = ['speechocean', 'phonemeref']\n",
    "        non_aug_df = df[df['source'].isin(non_aug_sources)]\n",
    "        if len(non_aug_df) >= 5:\n",
    "            test_samples = non_aug_df.sample(5, random_state=42)\n",
    "        else:\n",
    "            test_samples = df.sample(min(5, len(df)), random_state=42)\n",
    "else:\n",
    "    test_samples = df.sample(min(5, len(df)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30883e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Valutazione formale su test set\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = f'{DRIVE_OUTPUT_DIR}/final_model'\n",
    "if not Path(model_path).exists():\n",
    "    checkpoints = sorted([\n",
    "        d for d in Path(DRIVE_OUTPUT_DIR).iterdir() \n",
    "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
    "    ])\n",
    "    if checkpoints:\n",
    "        model_path = str(checkpoints[-1])\n",
    "\n",
    "print(f\"üìä Valutazione modello: {model_path}\")\n",
    "\n",
    "# Usa shell escaping corretto per path con spazi\n",
    "import subprocess\n",
    "result = subprocess.run([\n",
    "    \"python\", \"scripts/04_evaluate.py\",\n",
    "    \"--model-path\", model_path,\n",
    "    \"--test-csv\", DATASET_CSV,\n",
    "    \"--audio-base\", \".\"\n",
    "], capture_output=False)\n",
    "\n",
    "if result and result.returncode != 0:\n",
    "    print(\"‚ö†Ô∏è Errore nella valutazione\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696b9dd",
   "metadata": {},
   "source": [
    "## 6. Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Verifica contenuto su Drive\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ CONTENUTO SU GOOGLE DRIVE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Cartella: {DRIVE_OUTPUT_DIR}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "drive_path = Path(DRIVE_OUTPUT_DIR)\n",
    "if drive_path.exists():\n",
    "    for item in sorted(drive_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            n_files = len(list(item.rglob(\"*\")))\n",
    "            print(f\"  üìÅ {item.name}/ ({n_files} files)\")\n",
    "        else:\n",
    "            size_mb = item.stat().st_size / 1e6\n",
    "            print(f\"  üìÑ {item.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "    final_model = drive_path / \"final_model\"\n",
    "    if final_model.exists():\n",
    "        print(\"\\n‚úÖ Modello finale presente!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Modello finale non trovato\")\n",
    "else:\n",
    "    print(\"‚ùå Cartella non trovata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Crea zip per download\n",
    "import os\n",
    "\n",
    "FINAL_MODEL = f'{DRIVE_OUTPUT_DIR}/final_model'\n",
    "ZIP_PATH = f'{DRIVE_OUTPUT_DIR}/final_model.zip'\n",
    "\n",
    "if os.path.exists(FINAL_MODEL):\n",
    "    !cd {FINAL_MODEL} && zip -r {ZIP_PATH} .\n",
    "    print(f\"\\n‚úÖ Zip creato: {ZIP_PATH}\")\n",
    "    !ls -lh {ZIP_PATH}\n",
    "else:\n",
    "    print(\"‚ùå Modello finale non trovato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de91c2b",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Fine\n",
    "\n",
    "Il modello √® salvato su Google Drive:\n",
    "- `final_model/` - Modello trainato\n",
    "- `final_model.zip` - Per download rapido\n",
    "- `training_curves.png` - Grafici\n",
    "- `checkpoint-*/` - Checkpoint intermedi"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
