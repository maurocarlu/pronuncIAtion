# ü¶Å Model Zoo - Phoneme Recognition

Questo documento descrive in dettaglio tutte le architetture implementate e testate nel benchmark.

---

## 1. WavLM (Large)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `microsoft/wavlm-large` |
| **Piattaforma** | HuggingFace Transformers |
| **Parametri** | ~317M |
| **Pre-training** | Masked Speech Prediction + Denoising |

### Caratteristiche
WavLM estende HuBERT aggiungendo un task di **denoising** durante il pre-training. Maschera parti dell'input e aggiunge rumore/sovrapposizioni, costringendo il modello a imparare non solo la struttura fonetica ma anche caratteristiche acustiche robuste.

### Implementazione
- **Standard**: Fine-tuning classico CTC sull'ultimo layer.
- **Weighted**: Somma pesata dei 25 layer (vedi [Architecture Details](ARCHITECTURE_DETAILS.md)).

---

## 2. HuBERT (Large)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/hubert-large-ls960-ft` |
| **Parametri** | ~317M |
| **Pre-training** | Masked Prediction con target discreti (K-means clustering) |

### Caratteristiche
HuBERT impara mappando l'audio continuo a cluster discreti. Questo forza il modello a scartare dettagli acustici inutili (come il canale) e concentrarsi sulle unit√† linguistiche. √à stato il primo modello a dimostrare che ASL self-supervised scala bene.

---

## 3. XLS-R (300M)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/wav2vec2-xls-r-300m` |
| **Lingue** | 128 (Pre-training multilingua) |
| **Parametri** | ~300M |

### Caratteristiche
Versione multilingua di Wav2Vec2. Addestrato su mezza milionata di ore di audio in 128 lingue diverse.
**Motivazione**: Teoria che i modelli multilingua gestiscano meglio gli accenti non-nativi (come quelli in SpeechOcean762).

---

## 3b. XLS-R (1B) ‚≠ê NEW

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/wav2vec2-xlsr-1b` |
| **Lingue** | 128 (Pre-training multilingua) |
| **Parametri** | ~1B |
| **Input** | Raw Waveform (16kHz) |
| **Head** | CTC (`Wav2Vec2ForCTC`) |

### Caratteristiche
Versione ‚Äúscaled‚Äù di XLS-R per testare l‚Äôipotesi: pi√π capacit√† ‚Üí miglior AUC (detection) e/o PER su accenti non-nativi.

### Note di Implementazione
- **Vocab custom**: `data/processed/vocab.json`
- **Tokenizer**: `bos_token=None`, `eos_token=None`
- **Stabilit√† CTC**: `ctc_zero_infinity=True` + re-init `lm_head`
- **Memoria**: `fp16=True`, `gradient_checkpointing=True`, **auto 4-bit** se VRAM < 16GB
- **Monitoring**: `PredictionMonitorCallback` ogni 100 step

### Script
Vedi `scripts/training/train_xlsr_1b.py`.

---

## 3c. Data2Vec 2.0 (Large) ‚≠ê NEW

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/data2vec2-large-960h` |
| **Input** | Raw Waveform (16kHz) |
| **Head** | CTC (`Wav2Vec2ForCTC`) |
| **Pre-training** | Self-distillation (teacher/student) |

### Caratteristiche
Evoluzione di Wav2Vec2: tende a convergere pi√π rapidamente e con training pi√π stabile grazie all‚Äôobiettivo di self-distillation.

### Note di Implementazione
- **Vocab custom**: `data/processed/vocab.json`
- **Tokenizer**: `bos_token=None`, `eos_token=None`
- **Hyperparams benchmark**: LR `3e-5`, `warmup_ratio=0.1`, `gradient_accumulation_steps=4`
- **Memoria**: `fp16=True`, `gradient_checkpointing=True`
- **Monitoring**: `PredictionMonitorCallback` ogni 100 step

### Script
Vedi `scripts/training/train_data2vec2.py`.

---

## 4. Wav2Vec2 Phoneme (Domain Init)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/wav2vec2-xlsr-53-espeak-cv-ft` |
| **Input** | Raw Waveform (16kHz) |
| **Head** | CTC (`Wav2Vec2ForCTC`) |
| **Tokenizer** | `Wav2Vec2PhonemeCTCTokenizer` (vocab custom) |

### Caratteristiche
Questo modello usa l'architettura standard di Wav2Vec2 ma parte da un checkpoint basato su **XLSR-53** (multilingua) gi√† fine-tunato su fonemi (dataset CommonVoice + espeak).

**Motivazione scientifica**: "Test di un modello con inizializzazione specifica e multilingua per il dominio dei fonemi invece che SSL generico".

### Note di Implementazione
- **Vocab custom**: usa `data/processed/vocab.json`.
- **do_phonemize=False**: i target `ipa_clean` sono gi√† in formato fonetico.
- **CTC safety**: `bos_token` e `eos_token` sono impostati a `None` se non presenti nel vocab (evita mismatch negli indici della loss CTC).
- **Stabilit√†**: re-inizializzazione `lm_head` per ridurre rischio di blank collapse.
- **Monitoring**: `PredictionMonitorCallback` ogni 100 step.

### Script
Vedi `scripts/training/train_wav2vec2_phoneme.py`.

---

## 5. Whisper (Encoder Only)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `openai/whisper-small` (Encoder) |
| **Input** | Log-Mel Spectrogram (80 bins) |
| **Parametri** | ~87M (Encoder) + ~150M (Embeddings iniziali) |

### Architettura Custom
Whisper √® nativamente un Seq2Seq (Encoder-Decoder). Per il phoneme recognition frame-level:
1.  **Isoliamo l'Encoder**: Scartiamo il decoder autoregressivo.
2.  **CTC Head**: Aggiungiamo un layer lineare sopra gli hidden states dell'encoder.
3.  **Input**: Richiede conversione Audio ‚Üí Mel Spectrogram (non raw waveform).

### Implementazione
Vedi `scripts/training/train_whisper_encoder.py`.

---

## 6. SpeechTokenizer (Discrete)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `fnlp/SpeechTokenizer` (HuBERT-based) |
| **Tipo** | Discrete Token Representation |
| **Codebook** | 1024 token |

### Architettura a 2 Stadi
Approccio radicalmente diverso basato su token discreti.

1.  **Fase 1 (Encoder Frozen)**: L'audio passa per SpeechTokenizer che usa RVQ (Residual Vector Quantization) per produrre una sequenza di indici interi.
2.  **Fase 2 (Classifier Trainable)**: Un piccolo Transformer trainato da zero prende gli indici e predice i fonemi.

**Motivazione**: Testare se la discretizzazione "pulisce" il segnale mantenendo solo l'info fonetica.

---

## 7. Qwen2-Audio (Linear Probe)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `Qwen/Qwen2-Audio-7B-Instruct` (Audio Tower) |
| **Quantizzazione** | 4-bit (bitsandbytes NF4) |
| **Params Totali** | ~1B (FROZEN) |
| **Params Trainabili** | ~260k (CTC Head only) |
| **VRAM** | ~5-6GB |

### Caratteristiche
Modello multimodale (LLM + Audio). Estraiamo solo la "Audio Tower" pre-trainata.
**Modalit√†**: Linear Probe (encoder completamente frozen, solo CTC head trainabile).
**Obiettivo**: Valutare feature "zero-shot" del modello multimodale.

> **‚ö†Ô∏è Nota**: A causa dei requisiti computazionali (~10+ ore di training su GPU consumer), Qwen2-Audio viene trainato su **10.000 samples** invece del dataset completo (~40k).

---

## 8. Wav2Vec2-BERT 2.0 (‚≠ê Recommended)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/w2v-bert-2.0` |
| **Parametri** | ~600M |
| **Pre-training** | Contrastive + Masked Language Modeling |
| **Input** | Log-Mel Spectrogram (80 bins) |
| **VRAM** | ~10-12GB |

### Caratteristiche
Combina il contrastive learning di Wav2Vec2 con MLM di BERT.

> **‚ö†Ô∏è IMPORTANTE**: A differenza di Wav2Vec2/WavLM, W2V-BERT 2.0 richiede **spettrogrammi log-mel** (`input_features`), NON audio raw (`input_values`)!

### Configurazione Corretta
| Componente | Classe |
|------------|--------|
| Feature Extractor | `SeamlessM4TFeatureExtractor` (non Wav2Vec2FeatureExtractor!) |
| Processor | `Wav2Vec2BertProcessor` (non Wav2Vec2Processor!) |
| Model | `Wav2Vec2BertForCTC` |

### Training
- **LR**: 5e-6 (conservativo per evitare CTC collapse)
- **Warmup**: 2000 steps
- **Freeze**: `feature_projection` (non esiste `freeze_feature_encoder()`)
- **Subsampling**: Fattore 2 sui frame spettrogramma

---

## 9. MMS (Massively Multilingual Speech)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | `facebook/mms-1b-all` |
| **Parametri** | ~1B |
| **Pre-training** | 1000+ lingue, contrastive learning |
| **VRAM** | ~16GB (FP16) o ~8GB (4-bit) |

### Caratteristiche
MMS √® un modello massiccio (1 miliardo di parametri) addestrato su oltre 1000 lingue.
Usa adapter per lingua, ma qui lo usiamo con CTC head custom sul nostro vocab IPA.

### Configurazione
- **FP16**: Obbligatorio per VRAM
- **4-bit quantization**: Opzionale per GPU con <16GB VRAM
- **Batch Size**: 8 (con FP16)

**Motivazione**: Testare se il pre-training multilingue massivo migliora il riconoscimento fonetico.

### Script
- Benchmark script: `scripts/training/train_mms_1b.py`
- Script legacy/esteso: `scripts/training/train_mms.py`

---

## 9b. M-CTC-T (Meta) ‚≠ê NEW

| Parametro | Valore |
|-----------|--------|
| **Checkpoint** | `facebook/mctct-large` |
| **Input** | Mel Spectrogram (80 bins) |
| **Processor** | `MctctProcessor` |
| **Model** | `MctctForCTC` |

### Caratteristiche
M-CTC-T √® un modello CTC di Meta progettato per lavorare su **spettrogrammi Mel** (invece che su waveform raw).
Questo esperimento serve a confrontare in modo ‚Äúfair‚Äù l‚Äôefficacia di:
- **Mel-CTC (Meta)**: feature acustiche esplicite (Mel)
- **Raw-CTC (Meta)**: SSL su waveform (es. XLS-R, MMS)

### Note di Implementazione
- **Vocab custom**: `data/processed/vocab.json`
- **Tokenizer**: `bos_token=None`, `eos_token=None`
- **Init head**: re-init `lm_head` (std=0.02), `ignore_mismatched_sizes=True`
- **Hyperparams benchmark**: LR `3e-5`, `warmup_ratio=0.1`, `gradient_accumulation_steps=4`
- **Memoria**: `fp16=True`, `gradient_checkpointing=True`, **auto 4-bit** (linear probing: solo `lm_head`) se VRAM < 16GB
- **Monitoring**: `PredictionMonitorCallback` ogni 100 step

### Script
Vedi `scripts/training/train_mctct.py`.

---

## 10. Late Fusion (HuBERT + WavLM) ‚≠ê NEW

| Parametro | Valore |
|-----------|--------|
| **Modello A** | HuBERT Large (Best PER: 8.84%) |
| **Modello B** | WavLM Weighted (Best AUC: 0.8523) |
| **Strategia** | Logit-level ensemble |
| **VRAM** | ~16GB (2x Large models) |

### Formula
```
final_logits = Œ± √ó logits_HuBERT + (1-Œ±) √ó logits_WavLM
```

### Caratteristiche
Combina i due top-performer senza re-training:
- **HuBERT**: Pre-training con target discreti ‚Üí trascrizione precisa
- **WavLM Weighted**: Pre-training con denoising ‚Üí detection robusta

Pesi testati: Œ± ‚àà {0.3, 0.5, 0.7}

### Script
- Evaluation: `scripts/evaluation/evaluate_hubert_fusion.py`

---

## 11. Early Fusion (Multi-Backbone) ‚≠ê UPDATED

| Parametro | Valore |
|-----------|--------|
| **Backbone 1** | HuBERT Large (frozen, fine-tuned encoder) |
| **Backbone 2** | WavLM Base (frozen, fine-tuned encoder) |
| **Concatenazione** | 1024 + 768 = 1792D |
| **CTC Head** | Linear(1792, vocab_size) |
| **VRAM** | ~8-10GB (con 4-bit quantization) |

### Architettura
```
Audio ‚Üí HuBERT Large ‚Üí 1024D ‚îÄ‚îê
                               ‚îú‚Üí concat(1792D) ‚Üí CTC Head ‚Üí Phonemes
Audio ‚Üí WavLM Base  ‚Üí 768D ‚îÄ‚îÄ‚îò
```

### Caratteristiche
- Usa i **tuoi encoder gi√† fine-tuned** per phoneme recognition
- Solo la CTC head viene trainata (frozen backbones)
- Supporta 4-bit quantization per ridurre VRAM
- Il classificatore ha accesso simultaneo a rappresentazioni fonetiche (HuBERT) e acustiche (WavLM)

### Script
- Training: `scripts/training/train_early_fusion.py`
- Argomenti custom: `--wavlm-path` e `--hubert-path` per usare checkpoint fine-tuned


---

## 12. Baseline MLP (Linear Probe)

| Parametro | Valore |
|-----------|--------|
| **Backbone** | WavLM Base (Frozen) |
| **Aggregazione** | Global Average Pooling |
| **Classifier** | MLP (2 layer) |

### Caratteristiche
Modello "stupido" di controllo. Se un classificatore semplice su feature medie funziona bene, il task √® troppo facile. Serve come *lower bound*.

---

## Tabella Riassuntiva

| Modello | Input Type | Params | Training Mode | VRAM | Status | Script |
|---------|------------|--------|---------------|------|--------|--------|
| **HuBERT Large** | Raw Waveform | 317M | Fine-tuning | ~12GB | ‚úÖ **Best PER** | `train_hubert.py` |
| **WavLM Weighted** | Raw Waveform | 317M | Fine-tuning | ~12GB | ‚úÖ **Best AUC** | `train_weighted.py` |
| **Late Fusion** | Raw Waveform | 634M | Inference Only | ~16GB | üÜï NEW | `evaluate_hubert_fusion.py` |
| **Early Fusion** | Raw Waveform | 413M+2K | Frozen+CTC | ~8GB | üÜï UPDATED | `train_early_fusion.py` |
| WavLM Base/Large | Raw Waveform | 317M | Fine-tuning | ~12GB | ‚úÖ Works | `train_wavlm.py` |
| XLS-R 300M | Raw Waveform | 300M | Fine-tuning | ~10GB | ‚úÖ Works | `train_xlsr.py` |
| Baseline MLP | Raw Waveform | 2M train | Linear Probe | ~4GB | ‚úÖ Works | `train_baseline_mlp.py` |
| Whisper Small (Encoder) | Mel Spectrogram | 244M | Partial Fine-tuning | ~8GB | ‚ùå Failed | `train_whisper_encoder.py` |
| Wav2Vec2-BERT | Mel Spectrogram | 600M | Fine-tuning | ~12GB | ‚ùå Failed | `train_w2v2_bert.py` |
| Qwen2-Audio | Mel Spectrogram | 260K train | Linear Probe | ~5GB | ‚è≥ TBD | `train_qwen_audio.py` |
| SpeechTokenizer | Discrete Tokens | 256K train | 2-Stage | ~4GB | ‚ö†Ô∏è Partial | `train_speechtokenizer.py` |
| MMS 1B | Raw Waveform | 1B | Fine-tuning | ~16GB | ‚è≥ TBD | `train_mms.py` |

> **üí° Key Insight**: All models with **Raw Waveform** input (`input_values`) work well. Models requiring **Mel Spectrogram** (`input_features`) fail due to pre-training/CTC mismatch. See [ARCHITECTURE_DETAILS.md](ARCHITECTURE_DETAILS.md#8--input-types-vs-performance---critical-analysis) for detailed analysis.
