{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb1df1a",
   "metadata": {},
   "source": [
    "# üß† L-MAC Evaluation (SpeechOcean762)\n",
    "\n",
    "Questo notebook calcola **AI/AD** e genera esempi ascoltabili per L-MAC.\n",
    "\n",
    "**Supporta ambienti:**\n",
    "- üñ•Ô∏è Local\n",
    "- ‚òÅÔ∏è Google Colab  \n",
    "- üìä Kaggle (Dataset + Modelli da input)\n",
    "\n",
    "**Dataset:** SpeechOcean762 (full)  \n",
    "**Backbone:** HuBERT Large o Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec84ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Fix audio decoding: monkey-patch prima di qualsiasi uso di datasets\n",
    "import soundfile as sf\n",
    "import io\n",
    "import datasets\n",
    "import datasets.features.audio as audio_module\n",
    "\n",
    "def decode_audio_with_soundfile(self, value, token_per_repo_id=None):\n",
    "    \"\"\"Fallback audio decoder usando soundfile.\"\"\"\n",
    "    if isinstance(value, dict):\n",
    "        if \"bytes\" in value:\n",
    "            audio_bytes = value[\"bytes\"]\n",
    "            audio, sr = sf.read(io.BytesIO(audio_bytes))\n",
    "            return {\"array\": audio, \"sampling_rate\": sr, \"path\": value.get(\"path\", \"\")}\n",
    "        elif \"path\" in value:\n",
    "            audio, sr = sf.read(value[\"path\"])\n",
    "            return {\"array\": audio, \"sampling_rate\": sr, \"path\": value[\"path\"]}\n",
    "    return value\n",
    "\n",
    "audio_module.Audio.decode_example = decode_audio_with_soundfile\n",
    "print(\"‚úì Audio decoder patched to use soundfile\")\n",
    "\n",
    "def detect_environment():\n",
    "    if 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif '/kaggle' in os.getcwd() or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    return 'local'\n",
    "\n",
    "ENV = detect_environment()\n",
    "print(f'üñ•Ô∏è Ambiente: {ENV.upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b346c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies + clone repo\n",
    "pkgs = [\n",
    "    'transformers>=4.38',\n",
    "    'datasets>=2.18',\n",
    "    'evaluate',\n",
    "    'jiwer',\n",
    "    'soundfile',\n",
    "    'librosa',\n",
    "    'safetensors',\n",
    "    'accelerate',\n",
    "    'tqdm',\n",
    "    'pyyaml',\n",
    "    'pandas',\n",
    "]\n",
    "\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', *pkgs], check=False)\n",
    "\n",
    "import torch\n",
    "print(f'üî• PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üìä GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "# Clone repo\n",
    "IS_KAGGLE = Path('/kaggle').exists()\n",
    "SKIP_CLONE = str(os.environ.get('DL_PHONEME_SKIP_CLONE', '')).strip().lower() in ('1', 'true', 'yes')\n",
    "REPO_URL = 'https://github.com/maurocarlu/pronuncIAtion.git'\n",
    "PROJECT_DIR = Path('/kaggle/working/pronuncIAtion') if IS_KAGGLE else Path.cwd().parent.parent\n",
    "\n",
    "if IS_KAGGLE and (not SKIP_CLONE) and REPO_URL:\n",
    "    if not PROJECT_DIR.exists():\n",
    "        print('Cloning repo:', REPO_URL)\n",
    "        subprocess.run(['git', 'clone', REPO_URL, str(PROJECT_DIR)], check=False)\n",
    "    else:\n",
    "        print('Repo gi√† presente:', PROJECT_DIR)\n",
    "\n",
    "if PROJECT_DIR.exists():\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    sys.path.insert(0, str(PROJECT_DIR))\n",
    "print('CWD:', os.getcwd())\n",
    "print('PROJECT_DIR:', PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Kaggle: Symlink dati e path modelli ======\n",
    "DATA_INPUT = Path('/kaggle/input/pronunciation-data/data')\n",
    "DATA_TARGET = Path(PROJECT_DIR) / 'data'\n",
    "\n",
    "# Symlink data\n",
    "if Path('/kaggle').exists() and DATA_INPUT.exists():\n",
    "    try:\n",
    "        if not DATA_TARGET.exists():\n",
    "            os.symlink(str(DATA_INPUT), str(DATA_TARGET))\n",
    "            print('‚úì data symlink creato')\n",
    "    except Exception as e:\n",
    "        print('‚ö†Ô∏è Symlink fallito:', e)\n",
    "\n",
    "# ====== Model paths ======\n",
    "# Kaggle: modelli da input dataset\n",
    "KAGGLE_MODELS_PATH = Path('/kaggle/input/late-fusion/LateFusion')\n",
    "LOCAL_MODELS_PATH = PROJECT_DIR / 'outputs' / 'backup'\n",
    "\n",
    "if KAGGLE_MODELS_PATH.exists():\n",
    "    MODELS_ROOT = KAGGLE_MODELS_PATH\n",
    "    print(f'‚úì Using Kaggle models: {MODELS_ROOT}')\n",
    "else:\n",
    "    MODELS_ROOT = LOCAL_MODELS_PATH\n",
    "    print(f'‚úì Using local models: {MODELS_ROOT}')\n",
    "\n",
    "# Find available models\n",
    "print('\\nModelli disponibili:')\n",
    "for p in sorted(MODELS_ROOT.glob('**/config.json'))[:10]:\n",
    "    print(f'  ‚úì {p.parent.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9cfe5",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Configura:\n",
    "- `BACKBONE`: tipo di backbone (`hubert` o `early_fusion`)\n",
    "- `TARGET_PHONEME`: fonema IPA target per L-MAC\n",
    "- `MODEL_PATH`: path al modello fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "BACKBONE = \"hubert\"  # oppure \"early_fusion\"\n",
    "TARGET_PHONEME = None  # None = random phoneme sampling (multi-fonema)\n",
    "\n",
    "# Auto-detect model path based on environment\n",
    "if (MODELS_ROOT / 'final_model_hubert').exists():\n",
    "    MODEL_PATH = str(MODELS_ROOT / 'final_model_hubert')\n",
    "elif (MODELS_ROOT / 'hubert_large' / 'final_model_hubert').exists():\n",
    "    MODEL_PATH = str(MODELS_ROOT / 'hubert_large' / 'final_model_hubert')\n",
    "else:\n",
    "    # Fallback: cerca il primo modello con config.json\n",
    "    candidates = list(MODELS_ROOT.glob('**/config.json'))\n",
    "    MODEL_PATH = str(candidates[0].parent) if candidates else \"\"\n",
    "\n",
    "print(f'BACKBONE: {BACKBONE}')\n",
    "print(f'MODEL_PATH: {MODEL_PATH}')\n",
    "print(f'TARGET_PHONEME: {TARGET_PHONEME or \"None (multi-phoneme)\"}')\n",
    "\n",
    "# Auto-find decoder checkpoint (if already trained)\n",
    "# Per multi-fonema usa cartella \"multi\" invece del nome fonema\n",
    "phoneme_folder = TARGET_PHONEME if TARGET_PHONEME else \"multi\"\n",
    "decoder_root = PROJECT_DIR / \"outputs\" / \"lmac\" / BACKBONE / phoneme_folder\n",
    "candidates = sorted(decoder_root.glob(\"decoder_*.pt\"))\n",
    "DECODER_CKPT = str(candidates[-1]) if candidates else \"\"\n",
    "print(f'DECODER_CKPT: {DECODER_CKPT or \"Not found (will train)\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592437a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import dal progetto (gi√† in sys.path)\n",
    "from scripts.analysis.lmac_core import (\n",
    "    LMACBackboneConfig,\n",
    "    LMACSpeechOceanDataset,\n",
    "    LMACWrapper,\n",
    "    collate_audio_batch,\n",
    "    compute_ai_ad,\n",
    "    generate_listenable_map,\n",
    ")\n",
    "\n",
    "print('‚úì L-MAC imports loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c799af6",
   "metadata": {},
   "source": [
    "## üéØ Train Decoder (se non presente)\n",
    "\n",
    "Se il decoder L-MAC non √® gi√† stato trainato per il fonema target, lo alleniamo qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix audio decoding: usa soundfile invece di torchcodec\n",
    "import datasets\n",
    "datasets.config.TORCHCODEC_AVAILABLE = False\n",
    "\n",
    "# Se ancora non funziona, forza soundfile:\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_AUDIO_DECODER\"] = \"soundfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe033a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN (se decoder non presente) ===\n",
    "from types import SimpleNamespace\n",
    "from importlib import reload\n",
    "from scripts.analysis import train_lmac_decoder\n",
    "reload(train_lmac_decoder)  # Ricarica per avere le ultime modifiche\n",
    "\n",
    "if not DECODER_CKPT:\n",
    "    print('üèãÔ∏è Training L-MAC decoder...')\n",
    "    \n",
    "    # Se TARGET_PHONEME √® None, siamo in modalit√† multi-fonema \n",
    "    # e attiviamo il conditioning per permettere al modello di imparare\n",
    "    use_cond = (TARGET_PHONEME is None)\n",
    "    \n",
    "    args = SimpleNamespace(\n",
    "        model_path=MODEL_PATH,\n",
    "        backbone=BACKBONE,\n",
    "        target_phoneme=TARGET_PHONEME,\n",
    "        layer_ids=\"6,12,18,24\",\n",
    "        epochs=10,\n",
    "        batch_size=2,\n",
    "        lr=2e-4,\n",
    "        lambda_out=1.0,\n",
    "        lambda_reg=1e-4,\n",
    "        max_samples=None,\n",
    "        log_interval=50,\n",
    "        output_dir=str(PROJECT_DIR / \"outputs\" / \"lmac\"),\n",
    "        use_conditioning=use_cond, # <--- NUOVO PARAMETRO\n",
    "    )\n",
    "    train_lmac_decoder.train_lmac(args)\n",
    "    candidates = sorted(decoder_root.glob(\"decoder_*.pt\"))\n",
    "    DECODER_CKPT = str(candidates[-1]) if candidates else \"\"\n",
    "    print(f'‚úì Decoder trained: {DECODER_CKPT}')\n",
    "else:\n",
    "    print(f'‚úì Using existing decoder: {DECODER_CKPT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load decoder + backbone ===\n",
    "if not DECODER_CKPT:\n",
    "    raise FileNotFoundError(f\"Decoder checkpoint non trovato in {decoder_root}\")\n",
    "\n",
    "print(f\"Loading decoder from {DECODER_CKPT}...\")\n",
    "ckpt = torch.load(DECODER_CKPT, map_location=\"cpu\")\n",
    "\n",
    "# Extract config from checkpoint if available (backward compatibility)\n",
    "use_conditioning = ckpt.get(\"use_conditioning\", False)\n",
    "vocab_size = ckpt.get(\"vocab_size\", 0)\n",
    "\n",
    "print(f\"Configuration: Conditioning={use_conditioning}, VocabSize={vocab_size}\")\n",
    "\n",
    "config = LMACBackboneConfig(\n",
    "    backbone_type=BACKBONE,\n",
    "    model_path=MODEL_PATH,\n",
    "    layer_ids=(6, 12, 18, 24),\n",
    "    use_conditioning=use_conditioning,\n",
    "    vocab_size=vocab_size,\n",
    ")\n",
    "wrapper = LMACWrapper(config=config, target_phoneme=TARGET_PHONEME)\n",
    "wrapper.decoder.load_state_dict(ckpt[\"decoder_state\"])\n",
    "wrapper.eval()\n",
    "print('‚úì L-MAC Wrapper loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc99ab3",
   "metadata": {},
   "source": [
    "## üìä Evaluation: AI / AD Metrics\n",
    "\n",
    "Calcola le metriche **Attribution Intersection (AI)** e **Attribution Deletion (AD)** su SpeechOcean762."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AI / AD on SpeechOcean762 (test) ===\n",
    "print('üìä Computing AI/AD metrics on SpeechOcean762...')\n",
    "test_ds = LMACSpeechOceanDataset(split=\"test\", target_phoneme=TARGET_PHONEME, full=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, shuffle=False, collate_fn=collate_audio_batch)\n",
    "metrics = compute_ai_ad(wrapper, test_loader, max_batches=None)\n",
    "\n",
    "print('\\nüìà Global Results:')\n",
    "print(f'  AI: {metrics[\"AI\"]:.4f}%')\n",
    "print(f'  AD: {metrics[\"AD\"]:.4f}')\n",
    "\n",
    "if \"per_phoneme\" in metrics and metrics[\"per_phoneme\"]:\n",
    "    print('\\nüî¨ Per-Phoneme Breakdown (Top 10 by count):')\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_ph = sorted(\n",
    "        metrics[\"per_phoneme\"].items(), \n",
    "        key=lambda x: x[1]['count'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"{'Phoneme':<10} {'AI (%)':<10} {'AD':<10} {'Count':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for ph, stats in sorted_ph[:10]:\n",
    "        print(f\"{ph:<10} {stats['AI']:<10.2f} {stats['AD']:<10.4f} {stats['count']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a74b9",
   "metadata": {},
   "source": [
    "## üîä Listenable Maps\n",
    "\n",
    "Genera audio modificati per visualizzare/ascoltare le aree attribuite al fonema target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Esempio ascoltabile ===\n",
    "import random\n",
    "\n",
    "# Seleziona un audio dal dataset\n",
    "sample = test_ds[0]\n",
    "audio_path = None\n",
    "if isinstance(sample.get(\"audio\"), dict) and sample[\"audio\"].get(\"path\"):\n",
    "    audio_path = sample[\"audio\"][\"path\"]\n",
    "\n",
    "# Fallback: salva temporaneamente l'audio se non esiste path\n",
    "if audio_path is None:\n",
    "    import soundfile as sf\n",
    "    tmp_path = Path(PROJECT_DIR) / \"outputs\" / \"lmac\" / \"tmp_audio.wav\"\n",
    "    tmp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sf.write(tmp_path, sample[\"audio\"], 16000)\n",
    "    audio_path = str(tmp_path)\n",
    "\n",
    "# Logica Target:\n",
    "# 1. Se TARGET_PHONEME √® fissato (single mode), usiamo quello.\n",
    "# 2. Se √® None (multi mode), prendiamo un fonema REALE presente in questo audio.\n",
    "vis_target = TARGET_PHONEME\n",
    "if vis_target is None and wrapper.config.use_conditioning:\n",
    "    # Cerchiamo i fonemi presenti nella reference di questo sample\n",
    "    ref_ipa = sample.get(\"reference_ipa\", \"\")\n",
    "    if ref_ipa:\n",
    "        # Pulisci e splitta\n",
    "        candidates = list(set(ref_ipa.replace(' ', ''))) # Fonemi unici\n",
    "        # Filtra quelli non nel vocab\n",
    "        candidates = [c for c in candidates if c in wrapper.vocab]\n",
    "        if candidates:\n",
    "            vis_target = random.choice(candidates)\n",
    "            print(f\"‚ÑπÔ∏è Multi-Phoneme Mode: Auto-selected target '/{vis_target}/' for visualization\")\n",
    "\n",
    "out_dir = str(PROJECT_DIR / \"outputs\" / \"lmac\" / \"listenable_maps\")\n",
    "\n",
    "# Genera mappa (passando esplicitamente il target se serve)\n",
    "out = generate_listenable_map(wrapper, audio_path, out_dir=out_dir, target_phoneme=vis_target)\n",
    "print(f'\\nüîä Generated listenable map: {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Esempio ascoltabile ===\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Seleziona un audio dal dataset\n",
    "sample_idx = 0  # Cambia per vedere altri esempi\n",
    "sample = test_ds[sample_idx]\n",
    "\n",
    "print(f'\\nüìù === Sample {sample_idx} ===\\n')\n",
    "print(f'üìÑ Reference IPA: {sample.get(\"reference_ipa\", \"N/A\")}')\n",
    "print(f'üìÑ Text: {sample.get(\"text\", \"N/A\")}')\n",
    "print(f'üéØ Target Phoneme: {TARGET_PHONEME}')\n",
    "\n",
    "# Ottieni predizione del modello\n",
    "audio_arr = sample[\"audio\"]\n",
    "input_tensor = torch.tensor(audio_arr[None, :], dtype=torch.float32).to(wrapper.device)\n",
    "attn_mask = torch.ones_like(input_tensor, dtype=torch.long).to(wrapper.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    if wrapper.backbone_type == 'hubert':\n",
    "        out = wrapper.backbone(input_tensor, attention_mask=attn_mask, return_dict=True)\n",
    "        logits = out.logits\n",
    "    else:\n",
    "        out = wrapper.backbone(input_tensor, attention_mask=attn_mask)\n",
    "        logits = out['logits']\n",
    "    \n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "    # Decode prediction\n",
    "    pred_tokens = [wrapper.tokenizer.decode([tid.item()]) for tid in pred_ids if tid.item() != 0]\n",
    "    # Remove duplicates (CTC collapse)\n",
    "    collapsed = []\n",
    "    for t in pred_tokens:\n",
    "        if not collapsed or t != collapsed[-1]:\n",
    "            collapsed.append(t)\n",
    "    pred_text = ''.join(collapsed).replace('|', ' ').strip()\n",
    "\n",
    "print(f'\\nüîÆ Model Prediction: {pred_text}')\n",
    "\n",
    "# Salva audio temporaneo\n",
    "import soundfile as sf\n",
    "tmp_path = Path(PROJECT_DIR) / 'outputs' / 'lmac' / 'tmp_audio.wav'\n",
    "tmp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "sf.write(tmp_path, audio_arr, 16000)\n",
    "audio_path = str(tmp_path)\n",
    "\n",
    "# Genera mappa ascoltabile\n",
    "out_dir = str(PROJECT_DIR / 'outputs' / 'lmac' / 'listenable_maps')\n",
    "result = generate_listenable_map(wrapper, audio_path, out_dir=out_dir, prefix=f'sample_{sample_idx}')\n",
    "\n",
    "print(f'\\nüîä Generated files:')\n",
    "print(f'   - Masked audio: {result[\"masked_audio\"]}')\n",
    "print(f'   - Plot: {result[\"plot\"]}')\n",
    "\n",
    "# Visualizza immagine inline\n",
    "print(f'\\nüìä L-MAC Visualization:')\n",
    "img = Image.open(result['plot'])\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'L-MAC Mask for phoneme /{TARGET_PHONEME}/ - Sample {sample_idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Audio player (funziona su Jupyter/Kaggle)\n",
    "print('\\nüîà Original Audio:')\n",
    "display(Audio(audio_arr, rate=16000))\n",
    "\n",
    "print('\\nüîà Masked Audio (regions relevant for target phoneme):')\n",
    "masked_audio, _ = sf.read(result['masked_audio'])\n",
    "display(Audio(masked_audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Genera video animato con playhead E AUDIO sincronizzato ===\n",
    "!pip install moviepy -q\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "\n",
    "def create_lmac_video_with_audio(audio_arr, mask, output_path, fps=30):\n",
    "    \"\"\"Crea video MP4 con playhead e audio sincronizzato.\"\"\"\n",
    "    sr = 16000\n",
    "    duration = len(audio_arr) / sr\n",
    "    n_frames = int(duration * fps)\n",
    "    \n",
    "    # Crea la figura\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Waveform\n",
    "    time_axis = np.linspace(0, duration, len(audio_arr))\n",
    "    axes[0].plot(time_axis, audio_arr, color='#2c3e50', linewidth=0.5)\n",
    "    axes[0].set_xlim(0, duration)\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].set_title('Waveform')\n",
    "    line1 = axes[0].axvline(x=0, color='red', linewidth=2)\n",
    "    \n",
    "    # Plot 2: L-MAC Mask\n",
    "    mask_time = np.linspace(0, duration, len(mask))\n",
    "    axes[1].fill_between(mask_time, 0, mask, color='#e74c3c', alpha=0.7)\n",
    "    axes[1].plot(mask_time, mask, color='#c0392b', linewidth=1)\n",
    "    axes[1].set_xlim(0, duration)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].set_ylabel('Mask Value')\n",
    "    axes[1].set_title(f'L-MAC Mask for /{TARGET_PHONEME or \"multi\"}/')\n",
    "    line2 = axes[1].axvline(x=0, color='red', linewidth=2)\n",
    "    \n",
    "    # Plot 3: Spectrogram\n",
    "    try:\n",
    "        import librosa\n",
    "        import librosa.display\n",
    "        S = np.abs(librosa.stft(audio_arr, n_fft=512, hop_length=160))\n",
    "        S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "        librosa.display.specshow(S_db, sr=sr, hop_length=160, x_axis='time', y_axis='linear', ax=axes[2])\n",
    "    except:\n",
    "        from scipy.signal import spectrogram\n",
    "        f, t, Sxx = spectrogram(audio_arr, fs=sr)\n",
    "        axes[2].pcolormesh(t, f, 10*np.log10(Sxx+1e-9), shading='gouraud')\n",
    "    axes[2].set_title('Spectrogram')\n",
    "    line3 = axes[2].axvline(x=0, color='red', linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    def update(frame):\n",
    "        t = frame / fps\n",
    "        line1.set_xdata([t, t])\n",
    "        line2.set_xdata([t, t])\n",
    "        line3.set_xdata([t, t])\n",
    "        return line1, line2, line3\n",
    "    \n",
    "    # Salva video temporaneo (no audio)\n",
    "    temp_video = \"temp_video.mp4\"\n",
    "    anim = FuncAnimation(fig, update, frames=n_frames, interval=1000/fps, blit=True)\n",
    "    anim.save(temp_video, writer='ffmpeg', fps=fps, dpi=100)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Salva audio temporaneo\n",
    "    temp_audio = \"temp_audio.wav\"\n",
    "    import soundfile as sf\n",
    "    sf.write(temp_audio, audio_arr, sr)\n",
    "    \n",
    "    # Combina video + audio con moviepy\n",
    "    video_clip = VideoFileClip(temp_video)\n",
    "    audio_clip = AudioFileClip(temp_audio)\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    \n",
    "    final_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(temp_video)\n",
    "    os.remove(temp_audio)\n",
    "    return output_path\n",
    "\n",
    "# Genera video\n",
    "video_dir = Path(PROJECT_DIR) / 'outputs' / 'lmac' / 'videos'\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Logica Target per il Video (simile a listenable maps)\n",
    "vid_target = TARGET_PHONEME\n",
    "if vid_target is None and wrapper.config.use_conditioning:\n",
    "     # Cerca fonemi nell'audio corrente\n",
    "     ref_ipa = sample.get(\"reference_ipa\", \"\")\n",
    "     if ref_ipa:\n",
    "        candidates = [c for c in list(set(ref_ipa.replace(' ', ''))) if c in wrapper.vocab]\n",
    "        if candidates:\n",
    "            vid_target = candidates[0] # Prendi il primo per stabilit√† nel video\n",
    "            \n",
    "print(f\"üé¨ Generating video for target: /{vid_target or 'multi'}/...\")\n",
    "with torch.no_grad():\n",
    "    input_values = input_tensor\n",
    "    attention_mask = attn_mask\n",
    "    \n",
    "    target_ids = None\n",
    "    if wrapper.config.use_conditioning:\n",
    "         if vid_target:\n",
    "             tid = wrapper.vocab.get(vid_target, 0)\n",
    "             target_ids = torch.tensor([tid], device=wrapper.device, dtype=torch.long)\n",
    "         else:\n",
    "             target_ids = torch.tensor([0], device=wrapper.device, dtype=torch.long)\n",
    "    \n",
    "    out = wrapper.forward(input_values, attention_mask, target_ids=target_ids)\n",
    "    mask_arr = out[\"mask\"].cpu().numpy()[0]\n",
    "output_video = str(video_dir / f'lmac_sample_{sample_idx}.mp4')\n",
    "create_lmac_video_with_audio(audio_arr, mask_arr, output_video)\n",
    "# Visualizza nel notebook\n",
    "from IPython.display import Video\n",
    "display(Video(output_video, embed=True, width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c62d8",
   "metadata": {},
   "source": [
    "## üßπ Cleanup (Kaggle)\n",
    "\n",
    "Libera spazio disco rimuovendo cache HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3df369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup disk (Kaggle)\n",
    "import shutil\n",
    "if ENV == 'kaggle':\n",
    "    for f in ['/root/.cache/huggingface']:\n",
    "        if os.path.exists(f) and not os.path.islink(f):\n",
    "            shutil.rmtree(f)\n",
    "            print(f'üóëÔ∏è Cleaned: {f}')\n",
    "    # Check disk space\n",
    "    !df -h /kaggle/working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
