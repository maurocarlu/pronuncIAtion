{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† WavLM Weighted Layer Sum Training\n",
                "\n",
                "Questo notebook addestra WavLM con **Weighted Layer Sum**, un'architettura SOTA che combina tutti i 25 hidden states del Transformer con pesi apprendibili.\n",
                "\n",
                "**Vantaggi:**\n",
                "- Layer bassi: informazioni acustiche (formanti, pitch)\n",
                "- Layer alti: informazioni fonetiche/semantiche\n",
                "- Pesi apprendibili: il modello impara la combinazione ottimale"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Ambiente"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.1 Verifica GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.2 Monta Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print(\"‚úÖ Drive montato\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.3 Estrai progetto da zip\n",
                "import os\n",
                "import zipfile\n",
                "from pathlib import Path\n",
                "\n",
                "ZIP_PATH = '/content/drive/MyDrive/phonemeRef.zip'\n",
                "EXTRACT_PATH = '/content/DeepLearning-Phoneme'\n",
                "\n",
                "if not os.path.exists(ZIP_PATH):\n",
                "    raise FileNotFoundError(f\"‚ùå File non trovato: {ZIP_PATH}\\nCarica phonemeRef.zip su Google Drive\")\n",
                "\n",
                "print(f\"üì¶ Estrazione {ZIP_PATH}...\")\n",
                "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
                "    zip_ref.extractall('/content/')\n",
                "\n",
                "# Trova cartella estratta\n",
                "extracted = [f for f in os.listdir('/content/') if os.path.isdir(f'/content/{f}') and 'Phoneme' in f]\n",
                "if extracted:\n",
                "    EXTRACT_PATH = f'/content/{extracted[0]}'\n",
                "\n",
                "os.chdir(EXTRACT_PATH)\n",
                "print(f\"‚úÖ Progetto in: {EXTRACT_PATH}\")\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1.4 Installa dipendenze\n",
                "!pip install -q transformers datasets evaluate jiwer accelerate soundfile librosa pyyaml tqdm audiomentations\n",
                "!pip install -q torchcodec\n",
                "print(\"\\n‚úÖ Dipendenze installate\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preparazione Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Carica e analizza dataset\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "# Opzioni dataset\n",
                "DATASET_OPTIONS = [\n",
                "    'data/processed/combined_augmented.csv',\n",
                "    'data/processed/combined_dataset.csv',\n",
                "    'data/processed/phonemeref_processed.csv',\n",
                "]\n",
                "\n",
                "DATASET_CSV = None\n",
                "for opt in DATASET_OPTIONS:\n",
                "    if Path(opt).exists():\n",
                "        DATASET_CSV = opt\n",
                "        break\n",
                "\n",
                "if not DATASET_CSV:\n",
                "    raise FileNotFoundError(\"‚ùå Nessun dataset trovato!\")\n",
                "\n",
                "df = pd.read_csv(DATASET_CSV)\n",
                "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
                "print(f\"   Samples: {len(df):,}\")\n",
                "print(f\"\\n=== Distribuzione ===\")\n",
                "if 'source' in df.columns:\n",
                "    print(df['source'].value_counts())\n",
                "if 'is_correct' in df.columns:\n",
                "    print(f\"\\n=== Corretti vs Errori ===\")\n",
                "    print(df['is_correct'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.2 Verifica qualit√† IPA (cerca placeholder invalidi E annotazioni)\n",
                "import pandas as pd\n",
                "import json\n",
                "import re\n",
                "\n",
                "df = pd.read_csv(DATASET_CSV)\n",
                "\n",
                "# 1. Cerca IPA invalidi (placeholder [word])\n",
                "placeholder_mask = df['ipa_clean'].str.contains(r'^\\[.*\\]$', regex=True, na=False)\n",
                "\n",
                "# 2. Cerca annotazioni problematiche (adj., n., v., etc.)\n",
                "annotation_mask = df['ipa_clean'].str.contains(\n",
                "    r'adj\\.|n\\.|v\\.|adv\\.|interj\\.|for \\d|unstressed|stressed|esp\\.|also|Brit\\.|;',\n",
                "    regex=True, na=False\n",
                ")\n",
                "\n",
                "# 3. IPA troppo corti (< 2 caratteri)\n",
                "short_mask = df['ipa_clean'].str.len() < 2\n",
                "\n",
                "invalid_mask = placeholder_mask | annotation_mask | short_mask\n",
                "invalid_count = invalid_mask.sum()\n",
                "\n",
                "print(f\"üîç Analisi qualit√† IPA:\")\n",
                "print(f\"   Totale samples: {len(df):,}\")\n",
                "print(f\"   IPA placeholder [word]: {placeholder_mask.sum():,}\")\n",
                "print(f\"   IPA con annotazioni (adj., v., etc.): {annotation_mask.sum():,}\")\n",
                "print(f\"   IPA troppo corti (<2): {short_mask.sum():,}\")\n",
                "print(f\"   Totale invalidi: {invalid_count:,} ({100*invalid_count/len(df):.1f}%)\")\n",
                "\n",
                "if invalid_count > 0:\n",
                "    print(f\"\\n‚ö†Ô∏è ATTENZIONE: {invalid_count} samples hanno IPA problematici!\")\n",
                "    \n",
                "    # Mostra esempi\n",
                "    print(\"\\n   Esempi di IPA invalidi:\")\n",
                "    examples = df[invalid_mask][['word', 'ipa_clean']].head(10)\n",
                "    for _, row in examples.iterrows():\n",
                "        print(f\"   - {row['word']}: '{row['ipa_clean']}'\")\n",
                "    \n",
                "    # Rimuovi invalidi\n",
                "    df_clean = df[~invalid_mask].copy()\n",
                "    DATASET_CLEAN = 'data/processed/phonemeref_clean.csv'\n",
                "    df_clean.to_csv(DATASET_CLEAN, index=False)\n",
                "    print(f\"\\n‚úÖ Dataset pulito salvato: {DATASET_CLEAN}\")\n",
                "    print(f\"   Samples validi: {len(df_clean):,}\")\n",
                "    DATASET_CSV = DATASET_CLEAN\n",
                "else:\n",
                "    print(\"\\n‚úÖ Tutti gli IPA sono validi!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.3 Fix path e rimuovi file mancanti\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "df = pd.read_csv(DATASET_CSV)\n",
                "\n",
                "def fix_path(path_str):\n",
                "    \"\"\"Converte path Windows in path Colab.\"\"\"\n",
                "    path_str = str(path_str).replace('\\\\', '/')\n",
                "    \n",
                "    # Se √® gi√† un path relativo corretto (data/...), usalo\n",
                "    if path_str.startswith('data/'):\n",
                "        return path_str\n",
                "    \n",
                "    # Se inizia con 'audio/' (path relativo senza prefisso)\n",
                "    if path_str.startswith('audio/'):\n",
                "        return 'data/raw/phonemeref_data/' + path_str\n",
                "    \n",
                "    # Se contiene 'audio/' ma non 'data/', aggiungi il prefisso corretto\n",
                "    if '/audio/' in path_str:\n",
                "        idx = path_str.find('/audio/')\n",
                "        return 'data/raw/phonemeref_data' + path_str[idx:]\n",
                "    \n",
                "    # Se contiene path Windows assoluto con 'data/'\n",
                "    if 'data/' in path_str:\n",
                "        idx = path_str.find('data/')\n",
                "        return path_str[idx:]\n",
                "    \n",
                "    return path_str\n",
                "\n",
                "# Fix path\n",
                "df['audio_path'] = df['audio_path'].apply(fix_path)\n",
                "\n",
                "# === RIMUOVI FILE MANCANTI ===\n",
                "print(\"üîç Verifica esistenza file audio...\")\n",
                "missing_files = []\n",
                "existing_mask = []\n",
                "\n",
                "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Checking files\"):\n",
                "    exists = Path(row['audio_path']).exists()\n",
                "    existing_mask.append(exists)\n",
                "    if not exists:\n",
                "        missing_files.append((row.get('word', '?'), row['audio_path']))\n",
                "\n",
                "existing_mask = pd.Series(existing_mask, index=df.index)\n",
                "n_missing = len(missing_files)\n",
                "n_total = len(df)\n",
                "\n",
                "print(f\"\\nüìä Risultato verifica:\")\n",
                "print(f\"   Totale samples: {n_total:,}\")\n",
                "print(f\"   File esistenti: {n_total - n_missing:,} ({100*(n_total-n_missing)/n_total:.1f}%)\")\n",
                "print(f\"   File mancanti: {n_missing:,} ({100*n_missing/n_total:.1f}%)\")\n",
                "\n",
                "if n_missing > 0:\n",
                "    print(f\"\\n‚ö†Ô∏è Esempi file mancanti:\")\n",
                "    for word, path in missing_files[:10]:\n",
                "        print(f\"   ‚ùå {word}: {path}\")\n",
                "    \n",
                "    # Rimuovi file mancanti\n",
                "    df_clean = df[existing_mask].copy()\n",
                "    print(f\"\\n‚úÖ Rimossi {n_missing} samples con file mancanti\")\n",
                "    print(f\"   Dataset finale: {len(df_clean):,} samples\")\n",
                "    df = df_clean\n",
                "else:\n",
                "    print(\"\\n‚úÖ Tutti i file audio esistono!\")\n",
                "\n",
                "# Verifica distribuzione finale\n",
                "if 'source' in df.columns:\n",
                "    print(f\"\\nüìä Distribuzione finale:\")\n",
                "    print(df['source'].value_counts())\n",
                "\n",
                "# Salva\n",
                "DATASET_FINAL = 'data/processed/phonemeref_ready.csv'\n",
                "df.to_csv(DATASET_FINAL, index=False)\n",
                "print(f\"\\n‚úÖ Dataset pronto: {DATASET_FINAL}\")\n",
                "DATASET_CSV = DATASET_FINAL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.4 Verifica vocab.json\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "vocab_path = Path('data/processed/vocab.json')\n",
                "if vocab_path.exists():\n",
                "    with open(vocab_path, encoding='utf-8') as f:\n",
                "        vocab = json.load(f)\n",
                "    \n",
                "    print(f\"üìä Vocab: {len(vocab)} simboli\")\n",
                "    \n",
                "    # Caratteri speciali attesi\n",
                "    special = ['[PAD]', '[UNK]', '|']\n",
                "    \n",
                "    # Caratteri non-IPA problematici\n",
                "    non_ipa = []\n",
                "    ipa_chars = []\n",
                "    for char in vocab.keys():\n",
                "        if char in special:\n",
                "            continue\n",
                "        if len(char) == 1 and char.isalpha() and not char.isascii():\n",
                "            ipa_chars.append(char)\n",
                "        elif char in ['Àà', 'Àå', 'Àê', ' ≥', \"'\", '-', ' ']:  # Accenti e simboli IPA\n",
                "            ipa_chars.append(char)\n",
                "        elif char.lower() in 'abcdefghijklmnopqrstuvwxyz':  # Lettere ASCII (ok per IPA)\n",
                "            ipa_chars.append(char)\n",
                "        else:\n",
                "            non_ipa.append(char)\n",
                "    \n",
                "    print(f\"\\n   Caratteri speciali: {special}\")\n",
                "    print(f\"   Caratteri IPA: {len(ipa_chars)}\")\n",
                "    \n",
                "    if non_ipa:\n",
                "        print(f\"\\n   ‚ö†Ô∏è Caratteri sospetti: {non_ipa}\")\n",
                "    else:\n",
                "        print(f\"\\n   ‚úÖ Tutti i caratteri sembrano IPA validi\")\n",
                "    \n",
                "    print(f\"\\n   Esempio simboli: {list(vocab.keys())[3:15]}...\")\n",
                "else:\n",
                "    raise FileNotFoundError(\"‚ùå vocab.json non trovato!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configurazione Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.1 Configurazione (ottimizzata per Tesla T4)\n",
                "import yaml\n",
                "import os\n",
                "\n",
                "# === CONFIGURAZIONE PRINCIPALE ===\n",
                "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/phoneme_wavlm_weighted'\n",
                "\n",
                "config = {\n",
                "    'seed': 42,\n",
                "    'model': {\n",
                "        'name': 'microsoft/wavlm-large',\n",
                "        'freeze_feature_encoder': True\n",
                "    },\n",
                "    'data': {\n",
                "        'csv_path': DATASET_CSV,\n",
                "        'vocab_path': 'data/processed/vocab.json',\n",
                "        'audio_base_path': '.',\n",
                "        'val_size': 0.05,\n",
                "        'test_size': 0.05,\n",
                "        'sampling_rate': 16000\n",
                "    },\n",
                "    'training': {\n",
                "        'output_dir': DRIVE_OUTPUT_DIR,\n",
                "        'num_train_epochs': 10,\n",
                "        'per_device_train_batch_size': 8,\n",
                "        'per_device_eval_batch_size': 8,\n",
                "        'gradient_accumulation_steps': 2,\n",
                "        'dataloader_num_workers': 0,\n",
                "        'dataloader_pin_memory': False,\n",
                "        'learning_rate': 3e-5,\n",
                "        'warmup_steps': 500,\n",
                "        'weight_decay': 0.01,\n",
                "        'optim': 'adamw_torch',\n",
                "        'max_grad_norm': 1.0,\n",
                "        'fp16': True,\n",
                "        'bf16': False,\n",
                "        'eval_strategy': 'epoch',\n",
                "        'save_strategy': 'epoch',\n",
                "        'save_total_limit': 3,\n",
                "        'load_best_model_at_end': True,\n",
                "        'metric_for_best_model': 'per',\n",
                "        'greater_is_better': False,\n",
                "        'logging_steps': 100,\n",
                "        'disable_tqdm': False,\n",
                "        'group_by_length': True,\n",
                "    }\n",
                "}\n",
                "\n",
                "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Salva config\n",
                "with open('configs/training_config_weighted.yaml', 'w') as f:\n",
                "    yaml.dump(config, f, default_flow_style=False)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìã CONFIGURAZIONE WAVLM WEIGHTED (LARGE)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"üìÅ Output: {DRIVE_OUTPUT_DIR}\")\n",
                "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
                "print(f\"üî¢ Epochs: {config['training']['num_train_epochs']}\")\n",
                "print(f\"üì¶ Batch: {config['training']['per_device_train_batch_size']} x {config['training']['gradient_accumulation_steps']}\")\n",
                "print(f\"üìà LR: {config['training']['learning_rate']}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3.2 Verifica checkpoint esistenti\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "output_dir = Path(DRIVE_OUTPUT_DIR)\n",
                "checkpoints = []\n",
                "\n",
                "if output_dir.exists():\n",
                "    checkpoints = sorted([\n",
                "        d for d in output_dir.iterdir() \n",
                "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
                "    ])\n",
                "\n",
                "print(f\"üìÅ Output: {output_dir}\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "if checkpoints:\n",
                "    print(f\"‚úÖ {len(checkpoints)} checkpoint trovati:\")\n",
                "    \n",
                "    last_epoch = 0\n",
                "    best_per = None\n",
                "    \n",
                "    for cp in checkpoints[-3:]:\n",
                "        state_file = cp / \"trainer_state.json\"\n",
                "        if state_file.exists():\n",
                "            with open(state_file) as f:\n",
                "                state = json.load(f)\n",
                "            epoch = state.get('epoch', 0)\n",
                "            step = state.get('global_step', 0)\n",
                "            best = state.get('best_metric', None)\n",
                "            \n",
                "            last_epoch = max(last_epoch, epoch)\n",
                "            if best:\n",
                "                best_per = best\n",
                "            \n",
                "            info = f\"Epoch {epoch:.1f}, Step {step}\"\n",
                "            if best:\n",
                "                info += f\", Best PER: {best:.4f}\"\n",
                "            print(f\"   üìÅ {cp.name}: {info}\")\n",
                "    \n",
                "    target_epochs = config['training']['num_train_epochs']\n",
                "    if last_epoch >= target_epochs:\n",
                "        print(f\"\\n‚ö†Ô∏è TRAINING GI√Ä COMPLETATO! (epoch {last_epoch} >= {target_epochs})\")\n",
                "    else:\n",
                "        print(f\"\\n‚úÖ Training pu√≤ continuare per {target_epochs - last_epoch:.0f} epoche\")\n",
                "else:\n",
                "    print(\"‚ùå Nessun checkpoint - Training partir√† da zero\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4.1 Avvia Training con script train_weighted.py\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
                "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "\n",
                "# === OPZIONI ===\n",
                "RESUME = \"auto\"\n",
                "\n",
                "drive_path = Path(DRIVE_OUTPUT_DIR)\n",
                "existing_checkpoints = []\n",
                "if drive_path.exists():\n",
                "    existing_checkpoints = sorted([\n",
                "        d for d in drive_path.iterdir() \n",
                "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
                "    ])\n",
                "\n",
                "if RESUME == \"auto\":\n",
                "    do_resume = len(existing_checkpoints) > 0\n",
                "else:\n",
                "    do_resume = bool(RESUME)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ AVVIO TRAINING WAVLM WEIGHTED (LARGE)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"üìä Dataset: {DATASET_CSV}\")\n",
                "print(f\"üìÅ Output: {DRIVE_OUTPUT_DIR}\")\n",
                "print(f\"üîÑ Resume: {do_resume}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Comando\n",
                "cmd = f\"python scripts/training/train_weighted.py --config configs/training_config_weighted.yaml --data-csv {DATASET_CSV}\"\n",
                "if do_resume:\n",
                "    cmd += \" --resume\"\n",
                "\n",
                "!{cmd}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Valutazione"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.1 Visualizza curve di training\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "\n",
                "# Trova trainer_state.json\n",
                "state_path = None\n",
                "for loc in [\n",
                "    Path(DRIVE_OUTPUT_DIR) / 'final_model_weighted' / 'trainer_state.json',\n",
                "    Path(DRIVE_OUTPUT_DIR) / 'trainer_state.json',\n",
                "]:\n",
                "    if loc.exists():\n",
                "        state_path = loc\n",
                "        break\n",
                "\n",
                "# Cerca anche nell'ultimo checkpoint\n",
                "if not state_path:\n",
                "    checkpoints = sorted([\n",
                "        d for d in Path(DRIVE_OUTPUT_DIR).iterdir() \n",
                "        if d.is_dir() and d.name.startswith(\"checkpoint-\")\n",
                "    ]) if Path(DRIVE_OUTPUT_DIR).exists() else []\n",
                "    if checkpoints:\n",
                "        state_path = checkpoints[-1] / 'trainer_state.json'\n",
                "\n",
                "if state_path and state_path.exists():\n",
                "    with open(state_path) as f:\n",
                "        state = json.load(f)\n",
                "    \n",
                "    log_history = state.get('log_history', [])\n",
                "    \n",
                "    # Estrai metriche\n",
                "    train_loss = [(h['step'], h['loss']) for h in log_history if 'loss' in h and 'eval_loss' not in h]\n",
                "    eval_loss = [(h['step'], h['eval_loss']) for h in log_history if 'eval_loss' in h]\n",
                "    eval_per = [(h['step'], h['eval_per']) for h in log_history if 'eval_per' in h]\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "    \n",
                "    if train_loss:\n",
                "        steps, losses = zip(*train_loss)\n",
                "        axes[0].plot(steps, losses, 'b-', alpha=0.7)\n",
                "        axes[0].set_xlabel('Step')\n",
                "        axes[0].set_ylabel('Loss')\n",
                "        axes[0].set_title('Training Loss')\n",
                "        axes[0].grid(True, alpha=0.3)\n",
                "    \n",
                "    if eval_loss:\n",
                "        steps, losses = zip(*eval_loss)\n",
                "        axes[1].plot(steps, losses, 'r-o')\n",
                "        axes[1].set_xlabel('Step')\n",
                "        axes[1].set_ylabel('Eval Loss')\n",
                "        axes[1].set_title('Validation Loss')\n",
                "        axes[1].grid(True, alpha=0.3)\n",
                "    \n",
                "    if eval_per:\n",
                "        steps, pers = zip(*eval_per)\n",
                "        axes[2].plot(steps, [p*100 for p in pers], 'g-o')\n",
                "        axes[2].set_xlabel('Step')\n",
                "        axes[2].set_ylabel('PER (%)')\n",
                "        axes[2].set_title('Phoneme Error Rate')\n",
                "        axes[2].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f'{DRIVE_OUTPUT_DIR}/training_curves.png', dpi=150)\n",
                "    plt.show()\n",
                "    \n",
                "    if eval_per:\n",
                "        best_per = min(pers)\n",
                "        print(f\"\\nüèÜ Migliore PER: {best_per*100:.2f}%\")\n",
                "else:\n",
                "    print(\"‚ùå trainer_state.json non trovato - training non ancora completato?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.2 Valutazione su SpeechOcean762\n",
                "MODEL_PATH = f\"{DRIVE_OUTPUT_DIR}/final_model_weighted\"\n",
                "\n",
                "if Path(MODEL_PATH).exists():\n",
                "    print(f\"üî¨ Valutazione modello: {MODEL_PATH}\")\n",
                "    !python scripts/evaluation/evaluate_speechocean.py --model-path {MODEL_PATH}\n",
                "else:\n",
                "    print(f\"‚ö†Ô∏è Modello non trovato: {MODEL_PATH}\")\n",
                "    print(\"   Esegui prima il training!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5.3 Analisi Layer Weights (quali layer sono pi√π importanti)\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from pathlib import Path\n",
                "\n",
                "MODEL_PATH = f\"{DRIVE_OUTPUT_DIR}/final_model_weighted\"\n",
                "\n",
                "try:\n",
                "    # Carica il modello per vedere i pesi\n",
                "    checkpoint = torch.load(f\"{MODEL_PATH}/pytorch_model.bin\", map_location='cpu')\n",
                "    \n",
                "    if 'layer_weights' in checkpoint:\n",
                "        weights = checkpoint['layer_weights']\n",
                "        normalized = F.softmax(torch.tensor(weights), dim=0)\n",
                "        \n",
                "        print(\"üìä LAYER WEIGHTS (dopo training)\")\n",
                "        print(\"=\"*50)\n",
                "        for i, w in enumerate(normalized):\n",
                "            bar = \"‚ñà\" * int(w * 50)\n",
                "            print(f\"Layer {i:2d}: {w:.4f} {bar}\")\n",
                "        \n",
                "        print(f\"\\nüìä Layer pi√π importante: {normalized.argmax().item()}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è layer_weights non trovato nel checkpoint\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Errore caricamento: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Salvataggio Finale"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.1 Verifica contenuto su Drive\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìÅ CONTENUTO SU GOOGLE DRIVE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Cartella: {DRIVE_OUTPUT_DIR}\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "drive_path = Path(DRIVE_OUTPUT_DIR)\n",
                "if drive_path.exists():\n",
                "    for item in sorted(drive_path.iterdir()):\n",
                "        if item.is_dir():\n",
                "            n_files = len(list(item.rglob(\"*\")))\n",
                "            print(f\"  üìÅ {item.name}/ ({n_files} files)\")\n",
                "        else:\n",
                "            size_mb = item.stat().st_size / 1e6\n",
                "            print(f\"  üìÑ {item.name} ({size_mb:.1f} MB)\")\n",
                "\n",
                "    final_model = drive_path / \"final_model_weighted\"\n",
                "    if final_model.exists():\n",
                "        print(\"\\n‚úÖ Modello finale presente!\")\n",
                "    else:\n",
                "        print(\"\\n‚ö†Ô∏è Modello finale non trovato\")\n",
                "else:\n",
                "    print(\"‚ùå Cartella non trovata\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6.2 Crea zip per download\n",
                "import os\n",
                "\n",
                "FINAL_MODEL = f'{DRIVE_OUTPUT_DIR}/final_model_weighted'\n",
                "ZIP_PATH = f'{DRIVE_OUTPUT_DIR}/final_model_weighted.zip'\n",
                "\n",
                "if os.path.exists(FINAL_MODEL):\n",
                "    !cd {FINAL_MODEL} && zip -r {ZIP_PATH} .\n",
                "    print(f\"\\n‚úÖ Zip creato: {ZIP_PATH}\")\n",
                "    !ls -lh {ZIP_PATH}\n",
                "else:\n",
                "    print(\"‚ùå Modello finale non trovato\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üéâ Fine\n",
                "\n",
                "Il modello √® salvato su Google Drive:\n",
                "- `final_model_weighted/` - Modello trainato\n",
                "- `final_model_weighted.zip` - Per download rapido\n",
                "- `training_curves.png` - Grafici\n",
                "- `checkpoint-*/` - Checkpoint intermedi"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
