{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Unified Trainer - Phoneme Recognition Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, zipfile, glob, re, shutil\n",
    "\n",
    "def detect_environment():\n",
    "    if 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules:\n",
    "        return 'colab'\n",
    "    elif '/kaggle' in os.getcwd() or 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    return 'local'\n",
    "\n",
    "ENV = detect_environment()\n",
    "print(f'üñ•Ô∏è Ambiente: {ENV.upper()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB Setup\n",
    "if ENV == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    DRIVE_BACKUP = '/content/drive/MyDrive/phoneme_checkpoints'\n",
    "    PROJECT_DIR = '/content/DeepLearning-Phoneme'\n",
    "    ZIP_PATH = '/content/drive/MyDrive/DeepLearning-Phoneme.zip'\n",
    "    \n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "            z.extractall('/content')\n",
    "        print('‚úì Extracted')\n",
    "    else:\n",
    "        raise FileNotFoundError(ZIP_PATH)\n",
    "    \n",
    "    os.makedirs(DRIVE_BACKUP, exist_ok=True)\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    sys.path.insert(0, PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE Setup\n",
    "if ENV == 'kaggle':\n",
    "    PROJECT_DIR = '/kaggle/working/pronuncIAtion'\n",
    "    DRIVE_BACKUP = '/kaggle/working/checkpoints'\n",
    "    \n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        import subprocess\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/maurocarlu/pronuncIAtion.git', PROJECT_DIR])\n",
    "    \n",
    "    DATA_INPUT = '/kaggle/input/pronunciation-data/data'\n",
    "    DATA_TARGET = f'{PROJECT_DIR}/data'\n",
    "    \n",
    "    if os.path.islink(DATA_TARGET):\n",
    "        print('‚úì Data symlink exists')\n",
    "    elif os.path.exists(DATA_TARGET):\n",
    "        shutil.rmtree(DATA_TARGET)\n",
    "        os.symlink(DATA_INPUT, DATA_TARGET)\n",
    "    elif os.path.exists(DATA_INPUT):\n",
    "        os.symlink(DATA_INPUT, DATA_TARGET)\n",
    "    \n",
    "    os.makedirs(DRIVE_BACKUP, exist_ok=True)\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    sys.path.insert(0, PROJECT_DIR)\n",
    "    print(f'‚úì Kaggle ready: {PROJECT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL Setup\n",
    "if ENV == 'local':\n",
    "    PROJECT_DIR = os.getcwd()\n",
    "    if 'notebooks' in PROJECT_DIR:\n",
    "        PROJECT_DIR = os.path.dirname(PROJECT_DIR)\n",
    "    DRIVE_BACKUP = f'{PROJECT_DIR}/outputs'\n",
    "\n",
    "os.makedirs(DRIVE_BACKUP, exist_ok=True)\n",
    "os.chdir(PROJECT_DIR)\n",
    "sys.path.insert(0, PROJECT_DIR)\n",
    "print(f'üìÅ Project: {PROJECT_DIR}')\n",
    "print(f'üíæ Checkpoints: {DRIVE_BACKUP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'transformers', 'datasets', 'evaluate', 'jiwer', 'soundfile', 'librosa', 'bitsandbytes'])\n",
    "import torch\n",
    "print(f'üî• PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'csv_path': f'{PROJECT_DIR}/data/processed/combined_augmented.csv',\n",
    "    'vocab_path': f'{PROJECT_DIR}/data/processed/vocab.json',\n",
    "    'audio_base': PROJECT_DIR,\n",
    "    'epochs': 10,\n",
    "    'output_base': DRIVE_BACKUP,\n",
    "}\n",
    "\n",
    "for k,v in CONFIG.items():\n",
    "    if 'path' in k:\n",
    "        status = '‚úì' if os.path.exists(v) else '‚úó'\n",
    "        print(f'{status} {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "\n",
    "def run_streaming(cmd, cwd=None, env=None):\n",
    "    \"\"\"Run a command and stream stdout/stderr live in notebooks.\"\"\"\n",
    "    pretty = ' '.join(shlex.quote(str(x)) for x in cmd)\n",
    "    print(f'‚ñ∂ {pretty}')\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        cwd=cwd,\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "    assert proc.stdout is not None\n",
    "    for line in proc.stdout:\n",
    "        print(line, end='', flush=True)\n",
    "    rc = proc.wait()\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f'Command failed with exit code {rc}: {pretty}')\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. WAV2VEC2-BERT 2.0 (Recommended - Stable)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_w2v2_bert.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/w2v2_bert\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '8'\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MMS (legacy script: mms-300m / older setup)\n",
    "# Per MMS-1B usa la cella \"MMS-1B\" pi√π sotto.\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_mms.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/mms\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '8',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. WHISPER ENCODER\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_whisper_encoder.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/whisper_encoder\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '4'\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. QWEN2-AUDIO (Linear Probe - encoder frozen)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_qwen_audio.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/qwen_audio\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '2'\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SPEECHTOKENIZER\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_speechtokenizer.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/speechtokenizer\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '4'\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. WAV2VEC2 PHONEME (lv60-pmp - Domain Init)\n",
    "# NOTE: The training script now auto-detects char-level vocab and adds a [UNK] sanity check.\n",
    "extra = ['--save-to-drive'] if ENV == 'colab' else []\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_wav2vec2_phoneme.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    # Use a new folder to avoid overwriting an older run\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/wav2vec2_phoneme_charfix\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '4',\n",
    "    '--learning-rate', '3e-5',\n",
    "    # Fail fast if labels are mostly [UNK]\n",
    "    '--unk-check-samples', '300',\n",
    "    '--max-unk-ratio', '0.05',\n",
    "] + extra\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - New Benchmark Models\n",
    "\n",
    "> Queste celle lanciano i nuovi script aggiunti per il benchmark (raw waveform + M-CTC-T mel CTC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity run (consigliato)\n",
    "\n",
    "> Esegue 1 epoca con batch piccoli e salva in `outputs/_sanity/...` per verificare che pipeline/dataloader/model siano ok prima di lanciare run lunghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: DATA2VEC2 Large (1 epoca)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_data2vec2.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/_sanity/data2vec2_large\",\n",
    "    '--epochs', '1',\n",
    "    '--batch-size', '2',\n",
    "    # \"state dictionary corrupted\" su Kaggle = cache HF rotta: sblocca con force download\n",
    "    # '--force-download',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: XLS-R 1B (1 epoca)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_xlsr_1b.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/_sanity/xlsr_1b\",\n",
    "    '--epochs', '1',\n",
    "    '--batch-size', '1',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: MMS-1B (1 epoca)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_mms_1b.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/_sanity/mms_1b\",\n",
    "    '--epochs', '1',\n",
    "    '--batch-size', '1',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opzionale) Aggiorna Transformers\n",
    "# Nota: gli script del benchmark ora usano classi specifiche (NO AutoProcessor/AutoModel).\n",
    "# Se vedi ImportError su MCTCT* o Parakeet*, fai upgrade e riavvia il kernel (Colab/Kaggle).\n",
    "import transformers, sys\n",
    "print('transformers (prima):', transformers.__version__)\n",
    "!pip -q install --upgrade transformers\n",
    "import importlib\n",
    "importlib.reload(transformers)\n",
    "print('transformers (dopo):', transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: M-CTC-T (Meta) (1 epoca)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_mctct.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/_sanity/mctct_large\",\n",
    "    '--epochs', '1',\n",
    "    '--batch-size', '1',\n",
    "    # Se il checkpoint √® gated/privato (401), serve autenticazione HF.\n",
    "    # Opzione A (consigliata): imposta env var HUGGINGFACE_HUB_TOKEN / HF_TOKEN e rilancia la cella.\n",
    "    # Opzione B: passa esplicitamente il token (non committare il notebook con il token in chiaro!).\n",
    "    # '--hf-token', '<HF_TOKEN>',\n",
    "    # Se il download del checkpoint √® incompleto/corrotto, abilita il re-download\n",
    "    # '--force-download',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: Parakeet-CTC 1.1B (1 epoca)\n",
    "# Nota: il modello viene caricato obbligatoriamente in 4-bit (bitsandbytes richiesto).\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_parakeet.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/_sanity/parakeet_ctc_1p1b\",\n",
    "    '--epochs', '1',\n",
    "    '--batch-size', '1',\n",
    "    # Se il download del checkpoint √® incompleto/corrotto, abilita il re-download\n",
    "    # '--force-download',\n",
    "    # Se il checkpoint √® gated/privato (401), serve autenticazione HF\n",
    "    # '--hf-token', '<HF_TOKEN>',\n",
    " ]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. DATA2VEC2 Large (Raw Waveform)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_data2vec2.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/data2vec2_large\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '4',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. XLS-R 1B (Raw Waveform)\n",
    "# Suggerimento: se hai <16GB VRAM, lo script abilita 4-bit automaticamente.\n",
    "# Puoi forzare: aggiungi '--use-4bit' al cmd.\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_xlsr_1b.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/xlsr_1b\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '2',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. MMS-1B (Raw Waveform)\n",
    "# Suggerimento: se hai <16GB VRAM, lo script abilita 4-bit automaticamente.\n",
    "# Puoi forzare: aggiungi '--use-4bit' al cmd.\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_mms_1b.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/mms_1b\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '4',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. M-CTC-T (Meta) (Mel Spectrogram)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_mctct.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/mctct_large\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '2',\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Parakeet-CTC 1.1B (FastConformer-CTC)\n",
    "# Nota: carico 4-bit + backbone frozen + train solo lm_head (linear probing).\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/training/train_parakeet.py',\n",
    "    '--data-csv', CONFIG['csv_path'],\n",
    "    '--vocab-path', CONFIG['vocab_path'],\n",
    "    '--audio-base', CONFIG['audio_base'],\n",
    "    '--output-dir', f\"{CONFIG['output_base']}/parakeet_ctc_1p1b\",\n",
    "    '--epochs', str(CONFIG['epochs']),\n",
    "    '--batch-size', '1',\n",
    "    # Se il download del checkpoint √® incompleto/corrotto, abilita il re-download\n",
    "    # '--force-download',\n",
    "    # Se il checkpoint √® gated/privato (401), serve autenticazione HF\n",
    "    # '--hf-token', '<HF_TOKEN>',\n",
    " ]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (SpeechOcean762)\n",
    "\n",
    "Valuta il modello appena allenato su SpeechOcean762 con lo script di benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the model produced by the cell above\n",
    "model_path = f\"{CONFIG['output_base']}/wav2vec2_phoneme_charfix/final_model\"\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/evaluation/evaluate_speechocean.py',\n",
    "    '--model-path', model_path,\n",
    "    '--full',\n",
    "    # '--quiet',  # uncomment for shorter output\n",
    "]\n",
    "run_streaming(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup disk (Kaggle)\n",
    "if ENV == 'kaggle':\n",
    "    for f in ['/kaggle/working/checkpoints', '/root/.cache/huggingface']:\n",
    "        if os.path.exists(f) and not os.path.islink(f):\n",
    "            shutil.rmtree(f)\n",
    "            print(f'üóëÔ∏è {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoints as ZIP (Kaggle)\n",
    "if ENV == 'kaggle':\n",
    "    for model in [\n",
    "        'w2v2_bert',\n",
    "        'mms',\n",
    "        'whisper_encoder',\n",
    "        'qwen_audio',\n",
    "        'speechtokenizer',\n",
    "        'data2vec2_large',\n",
    "        'xlsr_1b',\n",
    "        'mms_1b',\n",
    "        'mctct_large',\n",
    "        'parakeet_ctc_1p1b',\n",
    "    ]:\n",
    "        p = f'{DRIVE_BACKUP}/{model}'\n",
    "        if os.path.exists(p):\n",
    "            shutil.make_archive(f'/kaggle/working/{model}_ckpt', 'zip', p)\n",
    "            print(f'‚úì {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Download checkpoints via browser (Colab only)\n",
    "if ENV == 'colab':\n",
    "    from google.colab import files\n",
    "    import datetime\n",
    "    \n",
    "    models_to_download = [\n",
    "        'w2v2_bert',\n",
    "        'mms',\n",
    "        'whisper_encoder',\n",
    "        'qwen_audio',\n",
    "        'speechtokenizer',\n",
    "        'data2vec2_large',\n",
    "        'xlsr_1b',\n",
    "        'mms_1b',\n",
    "        'mctct_large',\n",
    "        'parakeet_ctc_1p1b',\n",
    "    ]\n",
    "    \n",
    "    for model in models_to_download:\n",
    "        model_dir = f'{DRIVE_BACKUP}/{model}'\n",
    "        final_model = f'{model_dir}/final_model'\n",
    "        \n",
    "        # Prefer final_model if exists, otherwise use latest checkpoint\n",
    "        if os.path.exists(final_model):\n",
    "            source_dir = final_model\n",
    "            zip_name = f'{model}_final'\n",
    "        elif os.path.exists(model_dir):\n",
    "            # Find latest checkpoint\n",
    "            checkpoints = sorted(glob.glob(f'{model_dir}/checkpoint-*'), \n",
    "                               key=lambda x: int(x.split('-')[-1]) if x.split('-')[-1].isdigit() else 0)\n",
    "            if checkpoints:\n",
    "                source_dir = checkpoints[-1]\n",
    "                zip_name = f'{model}_{os.path.basename(source_dir)}'\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Create ZIP in /content (faster than Drive)\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "        zip_path = f'/content/{zip_name}_{timestamp}'\n",
    "        print(f'üì¶ Zipping {source_dir}...')\n",
    "        shutil.make_archive(zip_path, 'zip', source_dir)\n",
    "        zip_file = f'{zip_path}.zip'\n",
    "        size_mb = os.path.getsize(zip_file) / (1024*1024)\n",
    "        print(f'‚úì Created {zip_file} ({size_mb:.1f} MB)')\n",
    "        \n",
    "        # Trigger browser download\n",
    "        print('‚¨áÔ∏è Starting download...')\n",
    "        files.download(zip_file)\n",
    "        print(f'‚úì {model} download complete!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
